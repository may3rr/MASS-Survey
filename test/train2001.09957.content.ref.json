{
    "subject": [
        "Distributed, Parallel, and Cluster Computing",
        "Machine Learning",
        "Machine Learning"
    ],
    "reference": [
        "[1] Viktor Mauch, Marcel Kunze, and Marius Hillenbrand. High performance cloud computing. Future Generation Computer Systems, 29:1408 – 1416, 2013. Including Special sections: High Performance Computing in the Cloud & Resource Discovery Mechanisms for P2P Systems.  ",
        "[2] David A. Monge, Yisel Garí, Cristian Mateos, and Carlos García Garino. Autoscaling scientiﬁc workﬂows on the cloud by combining on-demand and spot instances. International Journal of Computer Systems Science and Engineering, 32(4 Special Issue on Elastic Data Management in Cloud Systems), Jul 2017.  ",
        "[3] Yisel Garí, David A. Monge, Cristian Mateos, and Carlos García Garino. Learning budget assignment policies for autoscaling scientiﬁc workﬂows in the cloud. Cluster Computing, Feb 2019.  ",
        "[4] Keven T Kearney and Francesco Torelli. Security in Service Level Agreements for Cloud Computing. Proceedings of the 1st International Conference on Cloud Computingand Services Science, (CLOSER), pages 636–642, 2011.  ",
        "[5] Xavier Dutreilh and Sergey Kirgizov. Using reinforcement learning for autonomic resource allocation in clouds: towards a fully automated workﬂow. In 7th International Conference on Autonomic and Autonomous Systems, pages 67–74, 2011.  ",
        "[6] Enda Barrett, Enda Howley, and Jim Duggan. Applying reinforcement learning towards automating resource allocation and application scalability in the cloud. Concurrency Computation Practice and Experience, 2012.  ",
        "[7] T. Veni and S. Mary Saira Bhanu. Auto-scale: automatic scaling of virtualised resources using neurofuzzy reinforcement learning approach. International Journal of Big Data Intelligence, 3, 2016.  ",
        "[8] Hamid Arabnejad, Claus Pahl, Pooyan Jamshidi, and Giovani Estrada. A comparison of reinforcement learning techniques for fuzzy cloud auto-scaling. In Proceedings 2017 17th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing, CCGRID 2017, pages 64–73, 2017.  ",
        "[9] Mostafa Ghobaei-Arani, Sam Jabbehdari, and Mohammad Ali Pourmina. An autonomic resource provisioning approach for service-based cloud applications: A hybrid approach. Future Generation Computer Systems, 78:191–210, 2018.  31  ",
        "[10] Naghmeh Dezhabad and Saeed Shariﬁan. Learning-based dynamic scalable load-balanced ﬁrewall as a service in network function-virtualized cloud computing environments. The Journal of Supercomputing, 2018.  ",
        "[11] J. V. Bibal Benifa and D. Dejey. RLPAS: Reinforcement Learning-based Proactive Auto-Scaler for Resource Provisioning in Cloud Environment. Mobile Networks and Applications, pages 1–16, 2018.  ",
        "[12] Enda Barrett, Enda Howley, and Jim Duggan. A learning architecture for scheduling workﬂow applications in the cloud. Proceedings – 9th IEEE European Conference on Web Services, ECOWS 2011, pages 83–90, 2011.  ",
        "[13] Zhiping Peng, Delong Cui, Jinglong Zuo, Qirui Li, Bo Xu, and Weiwei Lin. Random task scheduling scheme based on reinforcement learning in cloud computing. Cluster Computing, 18:1595–1607, 2015.  ",
        "[14] Zheng Xiao, Pijun Liang, Zhao Tong, Kenli Li, Samee U. Khan, and Keqin Li. Self-adaptation and mutual adaptation for distributed scheduling in benevolent clouds. Concurrency Computation, 29:1–12, 2017.  ",
        "[15] Martin Duggan, Jim Duggan, Enda Howley, and Enda Barrett. A network aware approach for the scheduling of virtual machine migration during peak loads. Cluster Computing, 20:2083–2094, Sep 2017.  ",
        "[16] Ning Liu, Zhe Li, Zhiyuan Xu, Jielong Xu, Sheng Lin, Qinru Qiu, Jian Tang, and Yanzhi Wang. A Hierarchical Framework of Cloud Resource Allocation and Power Management Using Deep Reinforcement Learning. In IEEE 37th International Conference on Distributed Computing Systems (ICDCS), pages 866–876, 2017.  ",
        "[17] M. Soualhia, F. Khomh, and S. Tahar. A Dynamic and Failure-aware Task Scheduling Framework for Hadoop. IEEE Transactions on Cloud Computing, 8:1–16, 2018.  ",
        "[18] Ji Li Mingxi Cheng and Shahin Nazarian. DRL-Cloud : Deep Reinforcement Learning-Based Resource Provisioning and Task Scheduling for Cloud Service Providers. In Design Automation Conference (ASP-DAC), 2018 23rd Asia and South Paciﬁc, pages 129–134, 2018.  ",
        "[19] Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction. A Bradford Book, USA, 2018.  ",
        "[20] David Silver, Aja Huang, Chris J. Maddison, Arthur Guez, Laurent Sifre, George Van Den Driessche, Julian Schrittwieser, Ioannis Antonoglou, Veda Panneershelvam, Marc Lanctot, Sander Dieleman, Dominik Grewe, John Nham, Nal Kalchbrenner, Ilya Sutskever, Timothy Lillicrap, Madeleine Leach, Koray Kavukcuoglu, Thore Graepel, and Demis Hassabis. Mastering the game of Go with deep neural networks and tree search. Nature, 529(7587):484–489, 2016.  ",
        "[21] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Bellemare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen, Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wierstra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning. Nature, 518:529 EP –, Feb 2015.  ",
        "[22] Jia Liu, Dustin Feld, Yong Xue, Jochen Garcke, Thomas Soddemann, and Peiyuan Pan. An efﬁcient geosciences workﬂow on multi-core processors and gpus: a case study for aerosol optical depth retrieval from modis satellite data. International Journal of Digital Earth, 9:748–765, 2016.  ",
        "[23] Duncan A. Brown, Patrick R. Brady, Alexander Dietz, Junwei Cao, Ben Johnson, and John McNabb. A Case Study on the Use of Workﬂow Technologies for Scientiﬁc Analysis: Gravitational Wave Data Analysis, pages 39–59. Springer London, London, 2007.  32  ",
        "[24] Bernard F Meade and Christopher J Fluke. Evaluating virtual hosted desktops for graphics-intensive astronomy. Astronomy and computing, 23:124–140, 2018.  ",
        "[25] Yves Vandenbrouck, David Christiany, Florence Combes, Valentin Loux, and Virginie Brun. Bioinformatics tools and workﬂow to select blood biomarkers for early cancer diagnosis: an application to pancreatic cancer. Proteomics, 19(21-22):1800489, 2019.  ",
        "[26] Henry Wang, Yunzhi Ma, Guillem Pratx, and Lei Xing. Toward real-time monte carlo simulation using a commercial cloud computing infrastructure. Physics in Medicine and Biology, 56:N175– N181, aug 2011.  ",
        "[27] Liqin Hu, Pengcheng Long, Jing Song, Tao He, Fang Wang, Lijuan Hao, Bin Wu, Shengpeng Yu, Peng He, Lei Wang, Guangyao Sun, Jun Zou, Quan Gan, Guomin Sun, Zhongyang Li, and Yican Wu. Supermc cloud for nuclear design and safety evaluation. Annals of Nuclear Energy, 134:424 – 431, 2019.  ",
        "[28] David A. Monge, Elina Pacini, Cristian Mateos, and Carlos García Garino. Meta-heuristic based autoscaling of cloud-based parameter sweep experiments with unreliable virtual machines instances. Computers and Electrical Engineering, 69:364–377, 2018.  ",
        "[29] Peter Mell, Timothy Grance, and Timothy Grance. The NIST Deﬁnition of Cloud Computing Recommendations of the National Institute of Standards and Technology. National Institute of Standards and Technology Special Publication 800-145 7, September 2011.  ",
        "[30] Y. Duan, G. Fu, N. Zhou, X. Sun, N. C. Narendra, and B. Hu. Everything as a service (xaas) on the cloud: Origins, current and future trends. In 2015 IEEE 8th International Conference on Cloud Computing, pages 621–628, June 2015.  ",
        "[31] G. Fortino, D. Parisi, V. Pirrone, and G. Di Fatta. BodyCloud: A SaaS approach for community Body Sensor Networks. Future Generation Computer Systems, 35:62–79, 2014.  ",
        "[32] Rajkumar Buyya, Chee Shin Yeo, Srikumar Venugopal, James Broberg, and Ivona Brandic. Cloud computing and emerging IT platforms: Vision, hype, and reality for delivering computing as the 5th utility. Future Generation Computer Systems, 25:599–616, 2009.  ",
        "[33] Younggyun Koh, Rob Knauerhase, Paul Brett, Mic Bowman, Wen Zhihua, and Calton Pu. An analysis of performance interference effects in virtual environments. In 2007 IEEE International Symposium on Performance Analysis of Systems Software, pages 200–209, April 2007.  ",
        "[34] Xing Pu, Ling Liu, Yiduo Mei, Sankaran Sivathanu, Younggyun Koh, and Calton Pu. Understanding performance interference of I/O workload in virtualized cloud environments. Proceedings 2010 IEEE 3rd International Conference on Cloud Computing, CLOUD 2010, pages 51–58, 2010.  ",
        "[35] Michael Armbrust, Armando Fox, Rean Grifﬁth, Anthony D Joseph, Randy Katz, Andy Konwinski, Gunho Lee, David Patterson, Ariel Rabkin, Ion Stoica, and Matei Zaharia. Above the clouds: A berkeley view of cloud computing. technical. Technical Report UCB/EECS-2009-28, EECS Department, University of California, Berkeley, 2009.  ",
        "[36] Javier Fabra, Joaquín Ezpeleta, and Pedro Álvarez. Reducing the price of resource provisioning using EC2 spot instances with prediction models. Future Generation Computer Systems, 96:348–367, 2019.  ",
        "[37] Jakub Krzywda, Ahmed Ali-Eldin, Trevor E. Carlson, Per-Plov Ostberg, and Erik Elmroth. Powerperformance tradeoffs in data center servers: DVFS, CPU pinning, horizontal, and vertical scaling. Future Generation Computer Systems, 81:114–128, 2018.  ",
        "[38] Ming Mao and Marty Humphrey. Scaling and scheduling to maximize application performance within budget constraints in cloud workﬂows. In Parallel & Distributed Processing (IPDPS), 2013 IEEE 27th International Symposium on, pages 67–78. IEEE, 2013.  33  ",
        "[39] David A. Monge, Elina Pacini, Cristian Mateos, Enrique Alba, and Carlos García Garino. CMI: An online multi-objective genetic autoscaler for scientiﬁc and engineering workﬂows in cloud infrastructures with unreliable virtual machines. Journal of Network and Computer Applications, 149:102464, 2020.  ",
        "[40] Elina Pacini, Cristian Mateos, and Carlos García Garino. Distributed job scheduling based on Swarm Intelligence: A survey. Computers & Electrical Engineering, 40:252–269, 2014. 40th-year commemorative issue.  ",
        "[41] Richard Bellman. Dynamic programming. Princeton University Press, New Jersey, 1957.  ",
        "[42] Martijn Van Otterlo. The Logic of Adaptive Behavior, volume 192 of Frontiers in Artiﬁcial Intelligence and Applications. IOS Press, Feb 2009.  ",
        "[43] Shaojie Tang, Jing Yuan, and Xiang Yang Li. Towards optimal bidding strategy for Amazon EC2 cloud spot instance. Proceedings – 2012 IEEE 5th International Conference on Cloud Computing, CLOUD 2012, pages 91–98, 2012.  ",
        "[44] Yisel Garí, David A. Monge, Cristian Mateos, and Carlos García Garino. Markov Decision Process to Dynamically Adapt Spots Instances Ratio on the Autoscaling of Scientiﬁc Workﬂows in the Cloud, pages 353–369. Springer International Publishing, Cham, 2018.  ",
        "[45] Joonhyouk Jang, Jinman Jung, and Jiman Hong. k-LZF : An efﬁcient and fair scheduling for Edge Computing servers. Future Generation Computer Systems, 98:44–53, 2019.  ",
        "[46] Ziqian Dong, Ning Liu, and Roberto Rojas-Cessa. Greedy scheduling of tasks with time constraints for energy-efﬁcient cloud-computing data centers. Journal of Cloud Computing, 4, 2015.  ",
        "[47] Shaminder Kaur and Amandeep Verma. An Efﬁcient Approach to Genetic Algorithm for Task Scheduling in Cloud Computing Environment. International Journal of Information Technology and Computer Science, 4:74–79, 2012.  ",
        "[48] Fatos Xhafa and Ajith Abraham. A compendium of heuristic methods for scheduling in computational grids. In Emilio Corchado and Hujun Yin, editors, Intelligent Data Engineering and Automated Learning IDEAL 2009, pages 751–758, Berlin, Heidelberg, 2009. Springer Berlin Heidelberg.  ",
        "[49] Anton Beloglazov and Rajkumar Buyya. Optimal online deterministic algorithms and adaptive heuristics for energy and performance efﬁcient dynamic consolidation of virtual machines in Cloud data centers. Concurrency and Computation: Practice and Experience, 24:1397–1420, 2012.  ",
        "[50] Kyong-Ha Lee, Yoon-Joon Lee, Hyunsik Choi, Yon Dohn Chung, and Bongki Moon. Parallel data processing with mapreduce: A survey. SIGMOD Rec., 40:11–20, January 2012.  ",
        "[51] Daria Glushkova, Petar Jovanovic, and Alberto Abelló. Mapreduce performance model for hadoop 2.x. Information Systems, 79:32–43, 2019. Special issue on DOLAP 2017: Design, Optimization, Languages and Analytical Processing of Big Data.  ",
        "[52] Jingqi Yang, Chuanchang Liu, Yanlei Shang, Bo Cheng, Zexiang Mao, Chunhong Liu, Lisha Niu, and Junliang Chen. A cost-aware auto-scaling approach using the workload prediction in service clouds. Information Systems Frontiers, 16:7–18, 2014.  ",
        "[53] Nilabja Roy, Abhishek Dubey, and Aniruddha Gokhale. Efﬁcient autoscaling in the cloud using predictive models for workload forecasting. Proceedings 2011 IEEE 4th International Conference on Cloud Computing, CLOUD 2011, pages 500–507, 2011.  ",
        "[54] Mahmoud Al-Ayyoub, Yaser Jararweh, Mustafa Daraghmeh, and Qutaibah Althebyan. Multi-agent based dynamic resource provisioning and monitoring for cloud computing systems infrastructure. Cluster Computing, 18:919–932, 2015.  34  ",
        "[55] Shair Horovitz and Yain Arian. Efﬁcient cloud auto-scaling with SLA Objective Using Q-Learning. In 6th International Conference on Future Internet of Things and Cloud (FiCloud), pages 85–92. IEEE, 2018.  ",
        "[56] Yi Wei, Daniel Kudenko, Shijun Liu, Li Pan, Lei Wu, and Xiangxiu Meng. A reinforcement learning based auto-scaling approach for saas providers in dynamic cloud environment. Mathematical Problems in Engineering, pages 1–11, 2019.  ",
        "[57] Seyed Mohammad Reza Nouri, Han Li, Srikumar Venugopal, Wenxia Guo, MingYun He, and Wenhong Tian. Autonomic decentralized elasticity based on a reinforcement learning controller for cloud applications. Future Generation Computer Systems, 94:765–780, 2019.  ",
        "[58] Gary Marcus. Deep Learning: A Critical Appraisal. arXiv e-prints, page arXiv:1801.00631, Jan 2018.  ",
        "[59] Leslie N. Smith. A disciplined approach to neural network hyper-parameters: Part 1 – learning rate, batch size, momentum, and weight decay. arXiv e-prints, page arXiv:1803.09820, Mar 2018.  ",
        "[60] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural Computation, 9:1735–1780, 1997.  ",
        "[61] Hongjia Li, J. Li, Wang Yao, S. Nazarian, X. Lin, and Y. Wang. Fast and energy-aware resource provisioning and task scheduling for cloud systems. In 2017 18th International Symposium on Quality Electronic Design (ISQED), pages 174–179, March 2017.  ",
        "[62] Zhiguang Wang, Chul Gwon, Tim Oates, and Adam Iezzi. Automated Cloud Provisioning on AWS using Deep Reinforcement Learning. arXiv e-prints, page arXiv:1709.04305, Sep 2017.  ",
        "[63] Pravesh Humane and JN Varshapriya. Simulation of cloud infrastructure using cloudsim simulator: A practical approach for researchers. In 2015 International Conference on Smart Technologies and Management for Computing, Communication, Controls, Energy and Materials (ICSTM), pages 207– 211. IEEE, 2015.  ",
        "[64] Ziyu Wang, Tom Schaul, Matteo Hessel, Hado van Hasselt, Marc Lanctot, and Nando de Freitas. Dueling Network Architectures for Deep Reinforcement Learning. arXiv e-prints, page arXiv:1511.06581, Nov 2015.  ",
        "[65] Zhao Tong, Hongjian Chen, Xiaomei Deng, Kenli Li, and Keqin Li. A scheduling scheme in the cloud computing environment using deep Q-learning. Information Sciences, 512:1170 – 1191, 2020.  ",
        "[66] Md Golam Rabiul Alam, Mohammad Hassan, Zla Uddin, Ahmad Almogren, and Giancarlo Fortino. Autonomic computation ofﬂoading in mobile edge for IoT applications. Future Generation Computer Systems, 90:149–157, 2019.  ",
        "[67] Bingqian Du, Chuan Wu, and Zhiyi Huang. Learning resource allocation and pricing for cloud proﬁt maximization. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pages 7570–7577, Honolulu,Hawaii, USA, 2019. AAAI Press.  ",
        "[68] Jia Rao, Xiangping Bu, Cheng-Zhong Xu, Leyi Wang, and George Yin. Vconf: A reinforcement learning approach to virtual machines auto-conﬁguration. In Proceedings of the 6th International Conference on Autonomic Computing, ICAC ’09, pages 137–146, New York, NY, USA, 2009. ACM.  ",
        "[69] Simon Spinner, Samuel Kounev, Xiaoyun Zhu, Lei Lu, Mustafa Uysal, Anne Holler, and Rean Grifﬁth. Runtime Vertical Scaling of Virtualized Applications via Online Model Estimation. International Conference on Self-Adaptive and Self-Organizing Systems, SASO, 2014-December(December):157– 166, 2014.  ",
        "[70] Nicos Makris. Plastic torsional buckling of cruciform compression members. Journal of Engineering Mechanics, 129:689–696, 2003.  35  ",
        "[71] Carlos García Garino, Melisa Ribero Vairo, Susana Andía Fagés, Anibal Mirasso, and Jean-Phillip Ponthot. Numerical simulation of ﬁnite strain viscoplastic problems. Journal of Computational and Applied Mathematics, 246:174–184, July 2013.  ",
        "[72] Saurabh Kumar Garg, Chee Shin Yeo, and Rajkumar Buyya. Green cloud framework for improving carbon efﬁciency of clouds. In Emmanuel Jeannot, Raymond Namyst, and Jean Roman, editors, EuroPar 2011 Parallel Processing, pages 491–502, Berlin, Heidelberg, 2011. Springer Berlin Heidelberg.  ",
        "[73] Anton Beloglazov, Rajkumar Buyya, Young Choon Lee, and Albert Zomaya. A Taxonomy and Survey of Energy-Efﬁcient Data Centers and Cloud Computing Systems, volume 82. Elsevier Inc., 1 edition, 2011.  ",
        "[74] Chunming Liu, Xin Xu, and Dewen Hu. Multiobjective reinforcement learning: A comprehensive overview. IEEE Transactions on Systems, Man, and Cybernetics: Systems, 45:385–398, March 2015.  ",
        "[75] Amazon. Autoescaling. http://aws.amazon.com/autoscaling/, January 2020. [Online; accessed May-2020].  ",
        "[76] Christopher J. C. H. Watkins and Peter Dayan. Q-learning. Machine Learning, 8(3-4):279–292, 1992.  ",
        "[77] Christoph Dann, Gerhard Neumann, and Jan Peters. Policy evaluation with temporal differences: A survey and comparison. Journal of Machine Learning Research, 15:809–883, 2014.  ",
        "[78] Pooyan Jamshidi, Amir Shariﬂoo, Claus Pahl, Hamid Arabnejad, Andreas Metzger, and Giovani Estrada. Fuzzy self-learning controllers for elasticity management in dynamic cloud architectures. Proceedings 2016 12th International ACM SIGSOFT Conference on Quality of Software Architectures, QoSA 2016, pages 70–79, 2016.  A.1 MDP Resolution via Dynamic Programming  Dynamic programming (DP) in this context refers to a collection of algorithms that can be used to compute optimal policies given a perfect model of the environment as an MDP. Methods based on DP compute the policy based on a complete model of the environment (Model-based). In the process of estimating the state values V (s), the probability distribution of the transitions between the states Pa(s, s′) is used. This often becomes a limitation, since it is not always possible to derive such a model. In some cases, the probability distribution of the transitions is estimated from data obtained in previous experiences. The DP methods offer an ofﬂine learning variant, where the policy is obtained by iterating over the model and not based on the dynamics of current experiences. It is important to note that prior estimates of other states are used in the process of estimating the values of the states (a technique known as bootstrap). Two widely used DP algorithms are policyIteration and valueIteration. Both algorithms have polynomial complexity in the number of states and actions, so it is important to consider the dimensions of these spaces when using DP. However, the search performed with DP is much more efﬁcient than an exhaustive exploration in the space of all possible policies. The policyIteration algorithm (see Algorithm 1) is deﬁned based on the iterative repetition of the evaluation and the improvement of the policy until convergence is achieved. In this way, the algorithm generates the following sequence of value functions and policies v0 → π0 → v1 → π1... → π∗ until to reach an appropriate policy. On the other hand, the valueIteration algorithm (see Algorithm 2), ﬁrst includes the search for the appropriate value function and then, the computation of the associated policy. These steps are not repeated because once the value function is adequate, so is the associated policy. The search for the appropriate value function can be understood as a combination of the policy improvement process and a truncated evaluation of the policy (the values are reassigned after a single sweep of the states) without losing convergence ",
        "[19]. In this way, the algorithm generates the sequence of value function updates v0 → v1 → v2 → ... → v∗ and then, it computes the suitable policy π∗.  36  Both algorithms, policyIteration and valueIteration, formally require an inﬁnite number of iterations to converge exactly to the appropriate policy. In practice, both algorithms stop when the difference between two successive approximations is less than a limit Θ, which usually within a much lesser number of iterations",
        "[19].  Algorithm 1 The Policy Iteration algorithm.  1: procedure POLICYITERATION(S, A, P, R, γ, Θ): 2: 1.Initialize V (s) y π(s) arbitrarily ∀s ∈ S  3: 2.Policy Evaluation 4: repeat: 5: ∆ ← 0 6: for each s ∈ S do 7: v ← V (s) 8: a ← π(s)  9: V (s) ← P s′ Pa(s, s′)[Ra(s, s′) + γV (s′)] 10: ∆ ← max(∆, |v − V (s)|)  11: until ∆ < Θ (a small positive number) 12: 3.Policy Improvement 13: stablePolicy ← true 14: for each s ∈ S do 15: oldAction ← π(s)  16: π(s) ← arg maxa P  s′ Pa(s, s′)[Ra(s, s′) + γV (s′)] 17: If oldAction ̸= π(s) then stablePolicy ← false  18: If stablePolicy then stop and return V ≈ v∗ and π ≈ π∗  Algorithm 2 The Value Iteration algorithm.  1: procedure VALUEITERATION(S, A, P, R, γ, Θ): 2: Initialize array V arbitrarily ∀s ∈ S 3: repeat: 4: ∆ ← 0  5: for each s ∈ S do 6: v ← V (s) 7: V (s) ← maxa P  s′ Pa(s, s′)[Ra(s, s′) + γV (s′)] 8: ∆ ← max(∆, |v − V (s)|)  9: until ∆ < Θ (a small positive number) 10: Output a deterministic policy π ≈ π∗ such that: 11: π(s) ← arg maxa P  s′ Pa(s, s′)[Ra(s, s′) + γV (s′)]  A.2 MDP Resolution via Temporal Difference Methods based on Temporal Difference (TD) do not require a perfect model of the environment (Modelfree), since the policy learning process is based on the observed dynamics during its experimentation. In this sense, these methods offer an approach with a greater ability to adapt to changes in the environment, since unlike DP-based methods, learning through TD occurs in an online way. Because of the lack of the model, the state value function V (s) is not sufﬁcient in suggesting a policy and it is required to estimate the values related to each action. The action-value function Q(s, a) represents the expected gain considering the state-action pair and it is usually represented in tabular form. Similarly to DP, to estimate new values, the previously estimated values are used (i.e., bootstrap is performed). Two widely used algorithms in this area are Q-learning ",
        "[76] and State-Action-Reward-State-Action (SARSA). It is important to highlight that one of the main limitations in RL is that the convergence time of these algorithms depends directly on the dimension of the state space and actions. Moreover, since these algorithms do not have an adequate initial policy they have a poor initial performance that will have  37  a greater or lesser impact depending on the addressed problem and the time taken for training. Making inappropriate decisions at the beginning of the autoscaling of workﬂows in Cloud, which is necessary for the exploration process, can have a direct impact on the makespan and the economic cost, so it is convenient to have an acceptable initial policy. This could also reduce the time required to learn the right policy. In any case, it is necessary to consider that obtaining an acceptable initial policy is not always trivial ",
        "[77]. The distinctive characteristic of Q-learning (see Algorithm 3) is that it uses two different policies, one to select the next action and another to update Q. In other words, Q-learning tries to evaluate π while following another policy µ. Alternatively, SARSA (see Algorithm 4) uses the same policy all the time. The most important difference between the two above mentioned algorithms is how Q is updated after each action. Q-learning updates Q with the action that maximizes the gain for the next step. This makes Qlearning follows an ε-greedy policy10 with ε = 0, i.e., there is no exploration. In contrast, SARSA updates Q by following exactly an ε-greedy policy, since the action is extracted from it. Both algorithms include the α ∈ (0, 1] parameter relative to the size of the step in the learning process, and the ε > 0 parameter that determines the exploration degree of new policies.  Algorithm 3 The Q-learning algorithm.  1: procedure Q-LEARNING(S, A, P, R, γ, α, ε): 2: Initialize Q(s, a) arbitrarily ∀s ∈ S, a ∈ A y Q(terminalState, .) = 0  3: for each (episode) do 4: Initialize S 5: repeat: 6: Select A from S using the policy derived from de Q (ε-greedy) 7: Take action A, observe R,S’ 8: Q(S, A) ← Q(S, A) + α[R + γ maxa Q(S′, a) − Q(S, A)]  9: S ← S′  10: until S is terminal  Algorithm 4 The SARSA algorithm.  1: procedure SARSA(S, A, P, R, γ, α, ε): 2: Initialize Q(s, a) arbitrarily ∀s ∈ S, a ∈ A y Q(terminalState, .) = 0 3: for each episode do 4: Initialize S 5: Select A from S using the policy derived from Q (ε-greedy)  6: repeat: 7: Take action A, observe R,S’ 8: Select A’ from S’ using the policy derived from Q (ε-greedy) 9: Q(S, A) ← Q(S, A) + α[R + γQ(S′, A′) − Q(S, A)] 10: S ← S′; A ← A′  11: until S is terminal  A.3 MDP Resolution via Neural Networks  Large state spaces in an RL problem leads to the need to ﬁnd non-tabular representations of the Q function, not only for the memory required to store large tables, but also for the time it would take to ﬁll it. Algorithms capable of generalizing in more complex and sophisticated state space contexts are then consequently needed. In this sense, non-linear approximations of Q with artiﬁcial neural networks appeared. A type of neural network that has proven very successful in RL applications [21, 20] is Deep Convolutional Neural Networks, which are specialized in the processing of large-scale data organized in spatial matrices. Then, the strategy that combines RL with Deep Neural Networks (DNN) is called  10ε-greedy: a policy that with an ε probability selects a random action, but most of the time it selects an action with the maximum estimated value.  38  Deep Reinforcement Learning (DRL). Finally, the DNN used to approximate the Q function is called DeepQNetworks (DQN) and the learning algorithm that uses DQN is referred to as Deep Q-learning. Figure 7 shows an example of a DQN. The input corresponds to a state s of the environment and the output represents the estimated value of function Q for the state s and all possible actions. In the process of training the network, the objective is to minimize the approximation error between the result of the network and the optimality equation of Bellman ",
        "[21]. Thereby, the same problem as with the classical DP and TD techniques is solved, but now using a nonlinear approach based on deep neural networks.  State Input Output  Q(s,a1)  Q(s,a2)  Q(s,a3)  Q(s,a4)  Hidden layers  S (v1, v2, ..., v8)  Figure 7: Example of the structure of a DQN.  A.4 MDP Resolution via Fuzzy Logic  Fuzzy Logic (FL) appears as another alternative to address the dimensionality problem of RL strategies. The idea is to reduce the state space using a diffuse representation of the information. Broadly, FL systems attempt to represent knowledge inaccurately, similar to how human beings do, and as opposed to classical numerical forms. In this sense, FL works with fuzzy sets in which the elements have some membership degree. To deﬁne the membership function of these sets, triangles or trapezoid curves are usually used (see Figure 8). For example, in Figure 8, a fuzzy membership function is represented for a Cloud workload variable with three fuzzy sets (Low, Medium, High) that deﬁne the membership degree of the variable to each of them. Thus, in the presence of a workload α, it is possible to afﬁrm that it belongs both to the Low and Medium fuzzy sets with a 50% of probability, and hence the diffuse nature of this representation.  Low Mid High  Workload  0  1α  Figure 8: Example of the fuzzy membership function (Y axis) for the workload variable. The fuzzy sets are deﬁned using a trapezoid.  These concepts from FL allow reasoning based on rules of the form:  if(antecedent)then(consequent),  39  where the antecedent and consequent values are expressed in a fuzzy way. Based on the previous example, one possible rule is: if the workload is high then more VMs must be allocated. FL has been applied in different ﬁelds, from Control Theory to Artiﬁcial Intelligence. A control process based on FL consists of the following steps:  • Mapping of input data to fuzzy set labels (Fuzziﬁer)  • The inference process based on fuzzy rules (Fuzzy Reasoning)  • Fuzzy output mapping to clear values (Defuzziﬁer)  Fuzzy Reinforcement Learning (FRL) is the strategy that combines the strength of fuzzy reasoning with RL. FRL allows handling problems with large state spaces without affecting the performance of the RL algorithms. For this, in the learning process a fuzzy representation of the information is used, which considerably reduces the number of states.  Automatic Controller  Fuzzy Q-learning  Fuzzy Logic  Reinforcement Learning  Monitoring Cloud Application Actuator  Fuzziﬁer  Inference Engine  Knowledge  Base  Defuzziﬁer  Cloud Platform  state S(v1,v2,.., vn)  action  Figure 9: Example of the architecture of a Cloud autoscaling system based on RL and FL. Figure adapted from ",
        "[78].  Motivated by this beneﬁt, some authors [8, 7] have proposed approaches based on FRL for autoscaling in Cloud. Figure 9 shows the interaction between the components involved in these approaches. First, the Cloud platform and the running application that composes the environment, which is continuously observed by a monitoring process. The monitoring process retrieves data of interest in the state of the environment and reports it to the Automatic Controller (AC). One of the main components of the AC is the FL-based control process called Fuzzy Controller (FC). The FC is composed of the Knowledge Base (or rules), the Fuzziﬁer, the Inference Engine and the Defuzziﬁer. In this way, the FC receives the signal of the environment state, transforms it to its diffuse representation, reasons based on the rules, and obtains a diffuse output that is ﬁnally returned to its clear representation. This output or action is used by the actuator process to modify the environment. The second component of the FC is precisely the RL process, which also receives the signal of the environment state and, guided by the optimization objectives, is responsible for learning the most appropriate set of rules to update the knowledge base of FC. Each member of the table of values Q is assigned to a speciﬁc rule (which describes some action-state pairs). Then, these values are updated during the learning process. In this way, it is possible to take advantage of the strengths of RL and FL strategies to design an automatic controller capable of evolving fuzzy rules that allow making approximate reasoning.  40 "
    ],
    "reference_content": [
        {
            "reference_num": "[1]",
            "reference_title": "High performance cloud computing",
            "reference_abstract": "Today’s high performance computing systems are typically managed and operated by individual organizations in private. Computing demand is fluctuating, however, resulting in periods where dedicated resources are either underutilized or overloaded. A cloud-based Infrastructure-as-a-Service (IaaS) approach for high performance computing applications promises cost savings and more flexibility. In this model virtualized and elastic resources are utilized on-demand from large cloud computing service providers to construct virtual clusters exactly matching a customer’s specific requirements. This paper gives an overview on the current state of high performance cloud computing technology and we describe the underlying virtualization techniques and management methods. Furthermore, we present a novel approach to use high speed cluster interconnects like InfiniBand in a high performance cloud computing environment."
        },
        {
            "reference_num": "[2]",
            "reference_title": "Autoscaling scientific workflows on the cloud by combining on-demand and spot instances",
            "reference_abstract": "Fil: Monge Bosdari, David Antonio. Consejo Nacional de Investigaciones Cientificas y Tecnicas. Centro Cientifico Tecnologico Conicet - Mendoza; Argentina. Universidad Nacional de Cuyo. Instituto para las Tecnologias de la Informacion y las Comunicaciones; Argentina"
        },
        {
            "reference_num": "[3]",
            "reference_title": "Learning budget assignment policies for autoscaling scientific workflows in the cloud",
            "reference_abstract": ""
        },
        {
            "reference_num": "[4]",
            "reference_title": "SECURITY IN SERVICE LEVEL AGREEMENTS FOR CLOUD COMPUTING",
            "reference_abstract": ""
        },
        {
            "reference_num": "[5]",
            "reference_title": "Using Reinforcement Learning for Autonomic Resource Allocation in Clouds: towards a fully automated workflow",
            "reference_abstract": "Dynamic and appropriate resource dimensioning isa crucial issue in cloud computing. As applications go more andmore 24/7, online policies must be sought to balance performancewith the cost of allocated virtual machines. Most industrialapproaches to date use ad hoc manual policies, such as thresholdbasedones. Providing good thresholds proved to be tricky andhard to automatize to fit every application requirement. Researchis being done to apply automatic decision-making approaches,such as reinforcement learning. Yet, they face a lot of problemsto go to the field: having good policies in the early phasesof learning, time for the learning to converge to an optimalpolicy and coping with changes in the application performancebehavior over time. In this paper, we propose to deal with theseproblems using appropriate initialization for the early stages aswell as convergence speedups applied throughout the learningphases and we present our first experimental results for these.We also introduce a performance model change detection onwhich we are currently working to complete the learning processmanagement. Even though some of these proposals were knownin the reinforcement learning field, the key contribution of thispaper is to integrate them in a real cloud controller and toprogram them as an automated workflow."
        },
        {
            "reference_num": "[6]",
            "reference_title": "Applying reinforcement learning towards automating resource allocation and application scalability in the cloud",
            "reference_abstract": "SUMMARY Public Infrastructure as a Service (IaaS) clouds such as Amazon, GoGrid and Rackspace deliver computational resources by means of virtualisation technologies. These technologies allow multiple independent virtual machines to reside in apparent isolation on the same physical host. Dynamically scaling applications running on IaaS clouds can lead to varied and unpredictable results because of the performance interference effects associated with co‐located virtual machines. Determining appropriate scaling policies in a dynamic non‐stationary environment is non‐trivial. One principle advantage exhibited by IaaS clouds over their traditional hosting counterparts is the ability to scale resources on‐demand. However, a problem arises concerning resource allocation as to which resources should be added and removed when the underlying performance of the resource is in a constant state of flux. Decision theoretic frameworks such as Markov Decision Processes are particularly suited to decision making under uncertainty. By applying a temporal difference, reinforcement learning algorithm known as Q‐learning, optimal scaling policies can be determined. Additionally, reinforcement learning techniques typically suffer from curse of dimensionality problems, where the state space grows exponentially with each additional state variable. To address this challenge, we also present a novel parallel Q‐learning approach aimed at reducing the time taken to determine optimal policies whilst learning online. Copyright © 2012 John Wiley &amp; Sons, Ltd."
        },
        {
            "reference_num": "[7]",
            "reference_title": "T. Veni and S. Mary Saira Bhanu. Auto-scale: automatic scaling of virtualised resources using neurofuzzy reinforcement learning approach. International Journal of Big Data Intelligence, 3, 2016.",
            "reference_abstract": "Database as a service provides services for accessing and managing customers\ndata which provides ease of access, and the cost is less for these services.\nThere is a possibility that the DBaaS service provider may not be trusted, and\ndata may be stored on untrusted server. The access control mechanism can\nrestrict users from unauthorized access, but in cloud environment access\ncontrol policies are more flexible. However, an attacker can gather sensitive\ninformation for a malicious purpose by abusing the privileges as another user\nand so database security is compromised. The other problems associated with the\nDBaaS are to manage role hierarchy and secure session management for query\ntransaction in the database. In this paper, a role-based access control for the\nmultitenant database with role hierarchy is proposed. The query is granted with\nleast access privileges, and a session key is used for session management. The\nproposed work protects data from privilege escalation and SQL injection. It\nuses the partial homomorphic encryption (Paillier Encryption) for the\nencrypting the sensitive data. If a query is to perform any operation on\nsensitive data, then extra permissions are required for accessing sensitive\ndata. Data confidentiality and integrity are achieved using the role-based\naccess control with partial homomorphic encryption."
        },
        {
            "reference_num": "[8]",
            "reference_title": "A Comparison of Reinforcement Learning Techniques for Fuzzy Cloud Auto-Scaling",
            "reference_abstract": "A goal of cloud service management is to design self-adaptable auto-scaler to react to workload fluctuations and changing the resources assigned. The key problem is how and when to add/remove resources in order to meet agreed service-level agreements. Reducing application cost and guaranteeing service-level agreements (SLAs) are two critical factors of dynamic controller design. In this paper, we compare two dynamic learning strategies based on a fuzzy logic system, which learns and modifies fuzzy scaling rules at runtime. A self-adaptive fuzzy logic controller is combined with two reinforcement learning (RL) approaches: (i) Fuzzy SARSA learning FSL and (ii) Fuzzy Q-learning FQL. As an off-policy approach, Q-learning learns independent of the policy currently followed, whereas SARSA as an on-policy always incorporates the actual agent's behavior and leads to faster learning. Both approaches are implemented and compared in their advantages and disadvantages, here in the OpenStack cloud platform. We demonstrate that both auto-scaling approaches can handle various load traffic situations, sudden and periodic, and delivering resources on demand while reducing operating costs and preventing SLA violations. The experimental results demonstrate that FSL and FQL have acceptable performance in terms of adjusted number of virtual machine targeted to optimize SLA compliance and response time."
        },
        {
            "reference_num": "[9]",
            "reference_title": "An autonomic resource provisioning approach for service-based cloud applications: A hybrid approach",
            "reference_abstract": "In cloud computing environment, resources can be dynamically provisioned on deman for cloud services The amount of the resources to be provisioned is determined during runtime according to the workload changes. Deciding the right amount of resources required to run the cloud services is not trivial, and it depends on the current workload of the cloud services. Therefore, it is necessary to predict the future demands to automatically provision resources in order to deal with fluctuating demands of the cloud services. In this paper, we propose a hybrid resource provisioning approach for cloud services that is based on a combination of the concept of the autonomic computing and the reinforcement learning (RL). Also, we present a framework for autonomic resource provisioning which is inspired by the cloud layer model. Finally, we evaluate the effectiveness of our approach under two real world workload traces. The experimental results show that the proposed approach reduces the total cost by up to 50%, and increases the resource utilization by up to 12% compared with the other approaches."
        },
        {
            "reference_num": "[10]",
            "reference_title": "Learning-based dynamic scalable load-balanced firewall as a service in network function-virtualized cloud computing environments",
            "reference_abstract": ""
        },
        {
            "reference_num": "[11]",
            "reference_title": "RLPAS: Reinforcement Learning-based Proactive Auto-Scaler for Resource Provisioning in Cloud Environment",
            "reference_abstract": ""
        },
        {
            "reference_num": "[12]",
            "reference_title": "A Learning Architecture for Scheduling Workflow Applications in the Cloud",
            "reference_abstract": "The scheduling of workflow applications involves the mapping of individual workflow tasks to computational resources, based on a range of functional and non-functional quality of service requirements. Workflow applications such as scientific workflows often require extensive computational processing and generate significant amounts of experimental data. The emergence of cloud computing has introduced a utility-type market model, where computational resources of varying capacities can be procured on demand, in a pay-per-use fashion. In workflow based applications dependencies exist amongst tasks which requires the generation of schedules in accordance with defined precedence constraints. These constraints pose a difficult planning problem, where tasks must be scheduled for execution only once all their parent tasks have completed. In general the two most important objectives of workflow schedulers are the minimisation of both cost and make span. The cost of workflow execution consists of both computational costs incurred from processing individual tasks, and data transmission costs. With scientific workflows potentially large amounts of data must be transferred between compute and storage sites. This paper proposes a novel cloud workflow scheduling approach which employs a Markov Decision Process to optimally guide the workflow execution process depending on environmental state. In addition the system employs a genetic algorithm to evolve workflow schedules. The overall architecture is presented, and initial results indicate the potential of this approach for developing viable workflow schedules on the Cloud."
        },
        {
            "reference_num": "[13]",
            "reference_title": "Random task scheduling scheme based on reinforcement learning in cloud computing",
            "reference_abstract": ""
        },
        {
            "reference_num": "[14]",
            "reference_title": "Self-adaptation and mutual adaptation for distributed scheduling in benevolent clouds",
            "reference_abstract": "SUMMARY Joint service involving several clouds is an emerging form of cloud computing. In hybrid clouds, the schedulers within 1 cloud must not only self‐adapt to the job arrival processes and the workload but also mutually adapt to the scheduling polices of other schedulers. However, as a combinatorial optimization problem, scheduling is challenged by the adaptation to those dynamics and uncertain behaviors of the peers. This article studies the collaboration among benevolent clouds that are cooperative in nature and willing to accept jobs from other clouds. We take advantage of machine learning and propose a distributed scheduling mechanism to learn the knowledge of job model, resource performance, and others' policies. Without explicit modeling and prediction, machine learning guides scheduling decisions based on experiences. To examine the performance of our approach, we conducted simulation using the SP2 job workload log of the San Diego Supercomputer Center under a test bed based on agent‐based systems—SWARM. The results validate that our approach has much shorter mean response time than 5 typical dynamic scheduling algorithms—opportunistic load balancing, minimum execution time, minimum completion time, switching algorithm, and k ‐percent best. A better collaboration in hybrid cloud is achieved by full adaptation."
        },
        {
            "reference_num": "[15]",
            "reference_title": "A network aware approach for the scheduling of virtual machine migration during peak loads",
            "reference_abstract": ""
        },
        {
            "reference_num": "[16]",
            "reference_title": "A Hierarchical Framework of Cloud Resource Allocation and Power Management Using Deep Reinforcement Learning",
            "reference_abstract": "Automatic decision-making approaches, such as reinforcement learning (RL), have been applied to (partially) solve the resource allocation problem adaptively in the cloud computing system. However, a complete cloud resource allocation framework exhibits high dimensions in state and action spaces, which prohibit the usefulness of traditional RL techniques. In addition, high power consumption has become one of the critical concerns in design and control of cloud computing systems, which degrades system reliability and increases cooling cost. An effective dynamic power management (DPM) policy should minimize power consumption while maintaining performance degradation within an acceptable level. Thus, a joint virtual machine (VM) resource allocation and power management framework is critical to the overall cloud computing system. Moreover, novel solution framework is necessary to address the even higher dimensions in state and action spaces. In this paper, we propose a novel hierarchical framework for solving the overall resource allocation and power management problem in cloud computing systems. The proposed hierarchical framework comprises a global tier for VM resource allocation to the servers and a local tier for distributed power management of local servers. The emerging deep reinforcement learning (DRL) technique, which can deal with complicated control problems with large state space, is adopted to solve the global tier problem. Furthermore, an autoencoder and a novel weight sharing structure are adopted to handle the high-dimensional state space and accelerate the convergence speed. On the other hand, the local tier of distributed server power managements comprises an LSTM based workload predictor and a model-free RL based power manager, operating in a distributed manner. Experiment results using actual Google cluster traces show that our proposed hierarchical framework significantly saves the power consumption and energy usage than the baseline while achieving no severe latency degradation. Meanwhile, the proposed framework can achieve the best trade-off between latency and power/energy consumption in a server cluster."
        },
        {
            "reference_num": "[17]",
            "reference_title": "A Dynamic and Failure-Aware Task Scheduling Framework for Hadoop",
            "reference_abstract": "Hadoop has become a popular framework for processing data-intensive applications in cloud environments. A core constituent of Hadoop is the scheduler, which is responsible for scheduling and monitoring the jobs and tasks, and rescheduling them in case of failures. Although fault-tolerance mechanisms have been proposed for Hadoop, the performance of Hadoop can be significantly impacted by unforeseen events in the cloud environment. In this paper, we introduce a dynamic and failure-aware framework that can be integrated within Hadoop scheduler and adjust the scheduling decisions based on collected information about the cloud environment. Our framework relies on predictions made by machine learning algorithms and scheduling policies generated by a Markovian Decision Process (MDP), to adjust its scheduling decisions on the fly. Instead of the fixed heartbeat-based failure detection commonly used in Hadoop to track active TaskTrackers (i.e., nodes that process the scheduled tasks), our proposed framework implements an adaptive algorithm that can dynamically detect the failures of the TaskTracker. To deploy our proposed framework, we have built, ATLAS+, an AdapTive Failure-Aware Scheduler for Hadoop. To assess the performance of ATLAS+, we conduct a large empirical study on a 100-node Hadoop cluster deployed on Amazon Elastic MapReduce (EMR), comparing the performance of ATLAS+ with those of three Hadoop schedulers (FIFO, Fair, and Capacity). Results show that ATLAS+ outperforms FIFO, Fair, and Capacity schedulers. ATLAS+ can reduce the number of failed jobs by up to 43 percent and the number of failed tasks by up to 59 percent. On average, ATLAS+ could reduce the total execution time of jobs by 10 minutes, which represents 40 percent of the job execution times, and by up to 3 minutes for tasks, which represents 47 percent of the task execution time. ATLAS+ also reduced CPU and memory usage by 22 and 20 percent, respectively."
        },
        {
            "reference_num": "[18]",
            "reference_title": "DRL-cloud: Deep reinforcement learning-based resource provisioning and task scheduling for cloud service providers",
            "reference_abstract": "Cloud computing has become an attractive computing paradigm in both academia and industry. Through virtualization technology, Cloud Service Providers (CSPs) that own data centers can structure physical servers into Virtual Machines (VMs) to provide services, resources, and infrastructures to users. Profit-driven CSPs charge users for service access and VM rental, and reduce power consumption and electric bills so as to increase profit margin. The key challenge faced by CSPs is data center energy cost minimization. Prior works proposed various algorithms to reduce energy cost through Resource Provisioning (RP) and/or Task Scheduling (TS). However, they have scalability issues or do not consider TS with task dependencies, which is a crucial factor that ensures correct parallel execution of tasks. This paper presents DRL-Cloud, a novel Deep Reinforcement Learning (DRL)-based RP and TS system, to minimize energy cost for large-scale CSPs with very large number of servers that receive enormous numbers of user requests per day. A deep Q-learning-based two-stage RP-TS processor is designed to automatically generate the best long-term decisions by learning from the changing environment such as user request patterns and realistic electric price. With training techniques such as target network, experience replay, and exploration and exploitation, the proposed DRL-Cloud achieves remarkably high energy cost efficiency, low reject rate as well as low runtime with fast convergence. Compared with one of the state-of-the-art energy efficient algorithms, the proposed DRL-Cloud achieves up to 320% energy cost efficiency improvement while maintaining lower reject rate on average. For an example CSP setup with 5,000 servers and 200,000 tasks, compared to a fast round-robin baseline, the proposed DRL-Cloud achieves up to 144% runtime reduction."
        },
        {
            "reference_num": "[19]",
            "reference_title": "Reinforcement Learning: An Introduction",
            "reference_abstract": "Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives when interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications. The only necessary mathematical background is familiarity with elementary concepts of probability. The book is divided into three parts. Part I defines the reinforcement learning problem in terms of Markov decision processes. Part II provides basic solution methods: dynamic programming, Monte Carlo methods, and temporal-difference learning. Part III presents a unified view of the solution methods and incorporates artificial neural networks, eligibility traces, and planning; the two final chapters present case studies and consider the future of reinforcement learning."
        },
        {
            "reference_num": "[21]",
            "reference_title": "Human-level control through deep reinforcement learning",
            "reference_abstract": ""
        },
        {
            "reference_num": "[22]",
            "reference_title": "An efficient geosciences workflow on multi-core processors and GPUs: a case study for aerosol optical depth retrieval from MODIS satellite data",
            "reference_abstract": "Quantitative remote sensing retrieval algorithms help understanding the dynamic aspects of Digital Earth. However, the Big Data and complex models in Digital Earth pose grand challenges for computation infrastructures. In this article, taking the aerosol optical depth (AOD) retrieval as a study case, we exploit parallel computing methods for high efficient geophysical parameter retrieval. We present an efficient geocomputation workflow for the AOD calculation from the Moderate Resolution Imaging Spectroradiometer (MODIS) satellite data. According to their individual potential for parallelization, several procedures were adapted and implemented for a successful parallel execution on multi-core processors and Graphics Processing Units (GPUs). The benchmarks in this paper validate the high parallel performance of the retrieval workflow with speedups of up to 5.x on a multi-core processor with 8 threads and 43.x on a GPU. To specifically address the time-consuming model retrieval part, hybrid parallel patterns which combine the multi-core processor’s and the GPU’s compute power were implemented with static and dynamic workload distributions and evaluated on two systems with different CPU–GPU configurations. It is shown that only the dynamic hybrid implementation leads to a greatly enhanced overall exploitation of the heterogeneous hardware environment in varying circumstances."
        },
        {
            "reference_num": "[23]",
            "reference_title": "A Case Study on the Use of Workflow Technologies for Scientific Analysis: Gravitational Wave Data Analysis",
            "reference_abstract": ""
        },
        {
            "reference_num": "[24]",
            "reference_title": "Evaluating virtual hosted desktops for graphics-intensive astronomy",
            "reference_abstract": "Visualisation of data is critical to understanding astronomical phenomena. Today, many instruments produce datasets that are too big to be downloaded to a local computer, yet many of the visualisation tools used by astronomers are deployed only on desktop computers. Cloud computing is increasingly used to provide a computation and simulation platform in astronomy, but it also offers great potential as a visualisation platform. Virtual hosted desktops, with graphics processing unit (GPU) acceleration, allow interactive, graphics-intensive desktop applications to operate co-located with astronomy datasets stored in remote data centres. By combining benchmarking and user experience testing, with a cohort of 20 astronomers, we investigate the viability of replacing physical desktop computers with virtual hosted desktops. In our work, we compare two Apple MacBook computers (one old and one new, representing hardware and opposite ends of the useful lifetime) with two virtual hosted desktops: one commercial (Amazon Web Services) and one in a private research cloud (the Australian NeCTAR Research Cloud). For two-dimensional image-based tasks and graphics-intensive three-dimensional operations – typical of astronomy visualisation workflows – we found that benchmarks do not necessarily provide the best indication of performance. When compared to typical laptop computers, virtual hosted desktops can provide a better user experience, even with lower performing graphics cards. We also found that virtual hosted desktops are equally simple to use, provide greater flexibility in choice of configuration, and may actually be a more cost-effective option for typical usage profiles."
        },
        {
            "reference_num": "[25]",
            "reference_title": "Bioinformatics Tools and Workflow to Select Blood Biomarkers for Early Cancer Diagnosis: An Application to Pancreatic Cancer",
            "reference_abstract": "Abstract Secretome proteomics for the discovery of cancer biomarkers holds great potential to improve early cancer diagnosis. A knowledge‐based approach relying on mechanistic criteria related to the type of cancer should help to identify candidates from available “omics” information. With the aim of accelerating the discovery process for novel biomarkers, a set of tools is developed and made available via a Galaxy‐based instance to assist end‐users biologists. These implemented tools proceed by a step‐by‐step strategy to mine transcriptomics and proteomics databases for information relating to tissue specificity, allow the selection of proteins that are part of the secretome, and combine this information with proteomics datasets to rank the most promising candidate biomarkers for early cancer diagnosis. Using pancreatic cancer as a case study, this strategy produces a list of 24 candidate biomarkers suitable for experimental assessment by MS‐based proteomics. Among these proteins, three (SYCN, REG1B, and PRSS2) were previously reported as circulating candidate biomarkers of pancreatic cancer. Here, further refinement of this list allows to prioritize 14 candidate biomarkers along with their associated proteotypic peptides for further investigation, using targeted MS‐based proteomics. The bioinformatics tools and the workflow implementing this strategy for the selection of candidate biomarkers are freely accessible at http://www.proteore.org ."
        },
        {
            "reference_num": "[26]",
            "reference_title": "Toward real-time Monte Carlo simulation using a commercial cloud computing infrastructure",
            "reference_abstract": "Monte Carlo (MC) methods are the gold standard for modeling photon and electron transport in a heterogeneous medium; however, their computational cost prohibits their routine use in the clinic. Cloud computing, wherein computing resources are allocated on-demand from a third party, is a new approach for high performance computing and is implemented to perform ultra-fast MC calculation in radiation therapy. We deployed the EGS5 MC package in a commercial cloud environment. Launched from a single local computer with Internet access, a Python script allocates a remote virtual cluster. A handshaking protocol designates master and worker nodes. The EGS5 binaries and the simulation data are initially loaded onto the master node. The simulation is then distributed among independent worker nodes via the message passing interface, and the results aggregated on the local computer for display and data analysis. The described approach is evaluated for pencil beams and broad beams of high-energy electrons and photons. The output of cloud-based MC simulation is identical to that produced by single-threaded implementation. For 1 million electrons, a simulation that takes 2.58 h on a local computer can be executed in 3.3 min on the cloud with 100 nodes, a 47× speed-up. Simulation time scales inversely with the number of parallel nodes. The parallelization overhead is also negligible for large simulations. Cloud computing represents one of the most important recent advances in supercomputing technology and provides a promising platform for substantially improved MC simulation. In addition to the significant speed up, cloud computing builds a layer of abstraction for high performance parallel computing, which may change the way dose calculations are performed and radiation treatment plans are completed."
        },
        {
            "reference_num": "[27]",
            "reference_title": "SuperMC cloud for nuclear design and safety evaluation",
            "reference_abstract": "For nuclear design and safety evaluation, along with the development of accuracy and efficiency, further demands are proposed for the nuclear simulation to computing platforms, such as convenient large data processing, secured data storage and transfer as well as secured control over the accessing to the codes. To meet the requirements, the SuperMC Cloud has been developed and has been designed to offer secure, efficient and convenient cloud service for nuclear simulation. It consists of two parts, the High Performance Calculation (HPC) part and the High Performance Graphics (HPG) part. Adopting advanced features dedicatedly designed for nuclear simulation, such as shared distributed storage, secured data storage and transfer, license control and etc., SuperMC cloud would be a preferred cloud platform for nuclear design and safety evaluation."
        },
        {
            "reference_num": "[28]",
            "reference_title": "Meta-heuristic based autoscaling of cloud-based parameter sweep experiments with unreliable virtual machines instances",
            "reference_abstract": "Cloud Computing is the delivery of on-demand computing resources over the Internet on a pay-per-use basis and is very useful to execute scientific experiments such as parameter sweep experiments (PSEs). When PSEs are executed it is important to reduce both the makespan and monetary cost. We propose a novel tri-objective formulation for the PSEs autoscaling problem considering unreliable virtual machines (VM) pursuing the minimization of makespan, monetary cost and probability of failures. We also propose a new autoscaler based on the Non-dominated Sorting Genetic Algorithm II able to automatically determine the right amount for each type of VM and pricing scheme, as well as the bid prices for the spot instances. Experiments show that the proposed autoscaler achieves great improvements in terms of makespan and cost when it is compared against Scaling First and Spot Instances Aware Autoscaling."
        },
        {
            "reference_num": "[29]",
            "reference_title": "Peter Mell, Timothy Grance, and Timothy Grance. The NIST Deﬁnition of Cloud Computing Recommendations of the National Institute of Standards and Technology. National Institute of Standards and Technology Special Publication 800-145 7, September 2011.",
            "reference_abstract": "Blockchains are tamper evident and tamper resistant digital ledgers\nimplemented in a distributed fashion (i.e., without a central repository) and\nusually without a central authority (i.e., a bank, company, or government). At\ntheir basic level, they enable a community of users to record transactions in a\nshared ledger within that community, such that under normal operation of the\nblockchain network no transaction can be changed once published. This document\nprovides a high-level technical overview of blockchain technology. The purpose\nis to help readers understand how blockchain technology works."
        },
        {
            "reference_num": "[30]",
            "reference_title": "Everything as a Service (XaaS) on the Cloud: Origins, Current and Future Trends",
            "reference_abstract": "For several years now, scientists have been proposing numerous models for defining anything \"as a service (aaS)\", including discussions of products, processes, data & information management, and security as a service. In this paper, based on a thorough literature survey, we investigate the vast stream of the state of the art in Everything as a Service (XaaS). We then use this investigation to explore an integrated view of XaaS that will help propose approaches for migrating applications to the cloud and exposing them as services."
        },
        {
            "reference_num": "[31]",
            "reference_title": "BodyCloud: A SaaS approach for community Body Sensor Networks",
            "reference_abstract": "Body Sensor Networks (BSNs) have been recently introduced for the remote monitoring of human activities in a broad range of application domains, such as health care, emergency management, fitness and behavior surveillance. BSNs can be deployed in a community of people and can generate large amounts of contextual data that require a scalable approach for storage, processing and analysis. Cloud computing can provide a flexible storage and processing infrastructure to perform both online and offline analysis of data streams generated in BSNs. This paper proposes BodyCloud, a SaaS approach for community BSNs that supports the development and deployment of Cloud-assisted BSN applications. BodyCloud is a multi-tier application-level architecture that integrates a Cloud computing platform and BSN data streams middleware. BodyCloud provides programming abstractions that allow the rapid development of community BSN applications. This work describes the general architecture of the proposed approach and presents a case study for the real-time monitoring and analysis of cardiac data streams of many individuals."
        },
        {
            "reference_num": "[32]",
            "reference_title": "Cloud computing and emerging IT platforms: Vision, hype, and reality for delivering computing as the 5th utility",
            "reference_abstract": "With the significant advances in Information and Communications Technology (ICT) over the last half century, there is an increasingly perceived vision that computing will one day be the 5th utility (after water, electricity, gas, and telephony). This computing utility, like all other four existing utilities, will provide the basic level of computing service that is considered essential to meet the everyday needs of the general community. To deliver this vision, a number of computing paradigms have been proposed, of which the latest one is known as Cloud computing. Hence, in this paper, we define Cloud computing and provide the architecture for creating Clouds with market-oriented resource allocation by leveraging technologies such as Virtual Machines (VMs). We also provide insights on market-based resource management strategies that encompass both customer-driven service management and computational risk management to sustain Service Level Agreement (SLA)-oriented resource allocation. In addition, we reveal our early thoughts on interconnecting Clouds for dynamically creating global Cloud exchanges and markets. Then, we present some representative Cloud platforms, especially those developed in industries, along with our current work towards realizing market-oriented resource allocation of Clouds as realized in Aneka enterprise Cloud technology. Furthermore, we highlight the difference between High Performance Computing (HPC) workload and Internet-based services workload. We also describe a meta-negotiation infrastructure to establish global Cloud exchanges and markets, and illustrate a case study of harnessing ‘Storage Clouds’ for high performance content delivery. Finally, we conclude with the need for convergence of competing IT paradigms to deliver our 21st century vision."
        },
        {
            "reference_num": "[33]",
            "reference_title": "An Analysis of Performance Interference Effects in Virtual Environments",
            "reference_abstract": "Virtualization is an essential technology in modern datacenters. Despite advantages such as security isolation, fault isolation, and environment isolation, current virtualization techniques do not provide effective performance isolation between virtual machines (VMs). Specifically, hidden contention for physical resources impacts performance differently in different workload configurations, causing significant variance in observed system throughput. To this end, characterizing workloads that generate performance interference is important in order to maximize overall utility. In this paper, we study the effects of performance interference by looking at system-level workload characteristics. In a physical host, we allocate two VMs, each of which runs a sample application chosen from a wide range of benchmark and real-world workloads. For each combination, we collect performance metrics and runtime characteristics using an instrumented Ken hypervisor. Through subsequent analysis of collected data, we identify clusters of applications that generate certain types of performance interference. Furthermore, we develop mathematical models to predict the performance of a new application from its workload characteristics. Our evaluation shows our techniques were able to predict performance with average error of approximately 5%"
        },
        {
            "reference_num": "[34]",
            "reference_title": "Understanding Performance Interference of I/O Workload in Virtualized Cloud Environments",
            "reference_abstract": "Server virtualization offers the ability to slice large, underutilized physical servers into smaller, parallel virtual machines (VMs), enabling diverse applications to run in isolated environments on a shared hardware platform. Effective management of virtualized cloud environments introduces new and unique challenges, such as efficient CPU scheduling for virtual machines, effective allocation of virtual machines to handle both CPU intensive and I/O intensive workloads. Although a fair number of research projects have dedicated to measuring, scheduling, and resource management of virtual machines, there still lacks of in-depth understanding of the performance factors that can impact the efficiency and effectiveness of resource multiplexing and resource scheduling among virtual machines. In this paper, we present our experimental study on the performance interference in parallel processing of CPU and network intensive workloads in the Xen Virtual Machine Monitors (VMMs). We conduct extensive experiments to measure the performance interference among VMs running network I/O workloads that are either CPU bound or network bound. Based on our experiments and observations, we conclude with four key findings that are critical to effective management of virtualized cloud environments for both cloud service providers and cloud consumers. First, running network-intensive workloads in isolated environments on a shared hardware platform can lead to high overheads due to extensive context switches and events in driver domain and VMM. Second, co-locating CPU-intensive workloads in isolated environments on a shared hardware platform can incur high CPU contention due to the demand for fast memory pages exchanges in I/O channel. Third, running CPU-intensive workloads and network-intensive workloads in conjunction incurs the least resource contention, delivering higher aggregate performance. Last but not the least, identifying factors that impact the total demand of the exchanged memory pages is critical to the in-depth understanding of the interference overheads in I/O channel in the driver domain and VMM."
        },
        {
            "reference_num": "[35]",
            "reference_title": "Michael Armbrust, Armando Fox, Rean Grifﬁth, Anthony D Joseph, Randy Katz, Andy Konwinski, Gunho Lee, David Patterson, Ariel Rabkin, Ion Stoica, and Matei Zaharia. Above the clouds: A berkeley view of cloud computing. technical. Technical Report UCB/EECS-2009-28, EECS Department, University of California, Berkeley, 2009.",
            "reference_abstract": "With the increasing commoditization of computer vision, speech recognition\nand machine translation systems and the widespread deployment of learning-based\nback-end technologies such as digital advertising and intelligent\ninfrastructures, AI (Artificial Intelligence) has moved from research labs to\nproduction. These changes have been made possible by unprecedented levels of\ndata and computation, by methodological advances in machine learning, by\ninnovations in systems software and architectures, and by the broad\naccessibility of these technologies.\n  The next generation of AI systems promises to accelerate these developments\nand increasingly impact our lives via frequent interactions and making (often\nmission-critical) decisions on our behalf, often in highly personalized\ncontexts. Realizing this promise, however, raises daunting challenges. In\nparticular, we need AI systems that make timely and safe decisions in\nunpredictable environments, that are robust against sophisticated adversaries,\nand that can process ever increasing amounts of data across organizations and\nindividuals without compromising confidentiality. These challenges will be\nexacerbated by the end of the Moore's Law, which will constrain the amount of\ndata these technologies can store and process. In this paper, we propose\nseveral open research directions in systems, architectures, and security that\ncan address these challenges and help unlock AI's potential to improve lives\nand society."
        },
        {
            "reference_num": "[36]",
            "reference_title": "Reducing the price of resource provisioning using EC2 spot instances with prediction models",
            "reference_abstract": "The increasing demand of computing resources has boosted the use of cloud computing providers. This has raised a new dimension in which the connections between resource usage and costs have to be considered from an organizational perspective. As a part of its EC2 service, Amazon introduced spot instances (SI) as a cheap public infrastructure, but at the price of not ensuring reliability of the service. On the Amazon SI model, hired instances can be abruptly terminated by the service provider when necessary. The interface for managing SI is based on a bidding strategy that depends on non-public Amazon pricing strategies, which makes complicated for users to apply any scheduling or resource provisioning strategy based on such (cheaper) resources. Although it is believed that the use of the EC2 SIs infrastructure can reduce costs for final users, a deep review of literature concludes that their characteristics and possibilities have not yet been deeply explored. In this work we present a framework for the analysis of the EC2 SIs infrastructure that uses the price history of such resources in order to classify the SI availability zones and then generate price prediction models adapted to each class. The proposed models are validated through a formal experimentation process. As a result, these models are applied to generate resource provisioning plans that get the optimal price when using the SI infrastructure in a real scenario. Finally, the recent changes that Amazon has introduced in the SI model and how this work can adapt to these changes is discussed."
        },
        {
            "reference_num": "[37]",
            "reference_title": "Jakub Krzywda, Ahmed Ali-Eldin, Trevor E. Carlson, Per-Plov Ostberg, and Erik Elmroth. Powerperformance tradeoffs in data center servers: DVFS, CPU pinning, horizontal, and vertical scaling. Future Generation Computer Systems, 81:114–128, 2018.",
            "reference_abstract": "Dynamic Voltage and Frequency Scaling (DVFS), CPU pinning, horizontal, and\nvertical scaling, are four techniques that have been proposed as actuators to\ncontrol the performance and energy consumption on data center servers. This\nwork investigates the utility of these four actuators, and quantifies the\npower-performance tradeoffs associated with them. Using replicas of the German\nWikipedia running on our local testbed, we perform a set of experiments to\nquantify the influence of DVFS, vertical and horizontal scaling, and CPU\npinning on end-to-end response time (average and tail), throughput, and power\nconsumption with different workloads. Results of the experiments show that DVFS\nrarely reduces the power consumption of underloaded servers by more than 5%,\nbut it can be used to limit the maximal power consumption of a saturated server\nby up to 20% (at a cost of performance degradation). CPU pinning reduces the\npower consumption of underloaded server (by up to 7%) at the cost of\nperformance degradation, which can be limited by choosing an appropriate CPU\npinning scheme. Horizontal and vertical scaling improves both the average and\ntail response time, but the improvement is not proportional to the amount of\nresources added. The load balancing strategy has a big impact on the tail\nresponse time of horizontally scaled applications."
        },
        {
            "reference_num": "[38]",
            "reference_title": "Scaling and Scheduling to Maximize Application Performance within Budget Constraints in Cloud Workflows",
            "reference_abstract": "It remains a challenge to provision resources in the cloud such that performance is maximized and financial cost is minimized. A fixed budget can be used to rent a wide variety of resource configurations for varying durations. The two steps - resource acquisition and scheduling/allocation - are dependent on each other and are particularly difficult when considering complex resource usage such as workflows, where task precedence need to be preserved and the budget constraint is assigned for the whole cloud application instead of every single job. The ability to acquire resources dynamically and trivially in the cloud - while being incredibly powerful and useful - exacerbates this particular resource acquisition and scheduling problem. In this paper, we design, implement and evaluate two auto-scaling solutions to minimize job turnaround time within budget constraints for cloud workflows. The scheduling-first algorithm distributes the application-wide budget to each individual job, determines the fastest execution plan and then acquires the cloud resources, while the scaling-first algorithm determines the size and the type of the cloud resources first and then schedules the workflow jobs on the acquired instances. The scaling-first algorithm shows better performance when the budget is low while the scheduling-first algorithm performs better when the budget is high. The two algorithms can reduce the job turnaround time by 9.6% - 45.2% compared to choosing a fixed general machine type. Moreover, they show good tolerance (between -10.2% and 16.7%) to inaccurate parameters (±20% estimation error)."
        },
        {
            "reference_num": "[39]",
            "reference_title": "CMI: An online multi-objective genetic autoscaler for scientific and engineering workflows in cloud infrastructures with unreliable virtual machines",
            "reference_abstract": "Cloud Computing is becoming the leading paradigm for executing scientific and engineering workflows. The large-scale nature of the experiments they model and their variable workloads make clouds the ideal execution environment due to prompt and elastic access to huge amounts of computing resources. Autoscalers are middleware-level software components that allow scaling up and down the computing platform by acquiring or terminating virtual machines (VM) at the time that workflow tasks are being scheduled. In this work we propose a novel online multi-objective autoscaler for workflows denominated Cloud Multi-objective Intelligence (CMI), which aims at the minimization of makespan, monetary cost and the potential impact of errors derived from unreliable VMs. Besides, this problem is subject to monetary budget constraints. CMI is responsible for periodically solving the autoscaling problems encountered along with the execution of a workflow. Simulation experiments on four well-known workflows exhibit that CMI significantly outperforms a state-of-the-art autoscaler of similar characteristics called Spot Instances Aware Autoscaling (SIAA). These results convey a solid base for deepening in the study of other meta-heuristic methods for autoscaling workflow applications using cheap but unreliable infrastructures."
        },
        {
            "reference_num": "[40]",
            "reference_title": "Distributed job scheduling based on Swarm Intelligence: A survey",
            "reference_abstract": "Scientists and engineers need computational power to satisfy the increasing resource intensive nature of their simulations. For example, running Parameter Sweep Experiments (PSE) involve processing many independent jobs, given by multiple initial configurations (input parameter values) against the same program code. Hence, paradigms like Grid Computing and Cloud Computing are employed for gaining scalability. However, job scheduling in Grid and Cloud environments represents a difficult issue since it is basically NP-complete. Thus, many variants based on approximation techniques, specially those from Swarm Intelligence (SI), have been proposed. These techniques have the ability of searching for problem solutions in a very efficient way. This paper surveys SI-based job scheduling algorithms for bag-of-tasks applications (such as PSEs) on distributed computing environments, and uniformly compares them based on a derived comparison framework. We also discuss open problems and future research in the area."
        },
        {
            "reference_num": "[41]",
            "reference_title": "Richard Bellman. Dynamic programming. Princeton University Press, New Jersey, 1957.",
            "reference_abstract": "Richard Bellman's Principle of Optimality, formulated in 1957, is the heart\nof dynamic programming, the mathematical discipline which studies the optimal\nsolution of multi-period decision problems. In this paper, we look at the main\ntrading principles of Jesse Livermore, the legendary stock operator whose\nmethod was published in 1923, from a Bellman point of view."
        },
        {
            "reference_num": "[42]",
            "reference_title": "Martijn Van Otterlo. The Logic of Adaptive Behavior, volume 192 of Frontiers in Artiﬁcial Intelligence and Applications. IOS Press, Feb 2009.",
            "reference_abstract": "We review the connection between statistical mechanics and the analysis of\nrandom optimization problems, with particular emphasis on the random k-SAT\nproblem. We discuss and characterize the different phase transitions that are\nmet in these problems, starting from basic concepts. We also discuss how\nstatistical mechanics methods can be used to investigate the behavior of local\nsearch and decimation based algorithms."
        },
        {
            "reference_num": "[43]",
            "reference_title": "Shaojie Tang, Jing Yuan, and Xiang Yang Li. Towards optimal bidding strategy for Amazon EC2 cloud spot instance. Proceedings – 2012 IEEE 5th International Conference on Cloud Computing, CLOUD 2012, pages 91–98, 2012.",
            "reference_abstract": "Amazon EC2 provides two most popular pricing schemes--i) the {\\em costly}\non-demand instance where the job is guaranteed to be completed, and ii) the\n{\\em cheap} spot instance where a job may be interrupted. We consider a user\ncan select a combination of on-demand and spot instances to finish a task. Thus\nhe needs to find the optimal bidding price for the spot-instance, and the\nportion of the job to be run on the on-demand instance. We formulate the\nproblem as an optimization problem and seek to find the optimal solution. We\nconsider three bidding strategies: one-time requests with expected guarantee\nand one-time requests with penalty for incomplete job and violating the\ndeadline, and persistent requests. Even without a penalty on incomplete jobs,\nthe optimization problem turns out to be non-convex. Nevertheless, we show that\nthe portion of the job to be run on the on-demand instance is at most half. If\nthe job has a higher execution time or smaller deadline, the bidding price is\nhigher and vice versa. Additionally, the user never selects the on-demand\ninstance if the execution time is smaller than the deadline.\n  The numerical results illustrate the sensitivity of the effective portfolio\nto several of the parameters involved in the model. Our empirical analysis on\nthe Amazon EC2 data shows that our strategies can be employed on the real\ninstances, where the expected total cost of the proposed scheme decreases over\n45\\% compared to the baseline strategy."
        },
        {
            "reference_num": "[44]",
            "reference_title": "Yisel Garí, David A. Monge, Cristian Mateos, and Carlos García Garino. Markov Decision Process to Dynamically Adapt Spots Instances Ratio on the Autoscaling of Scientiﬁc Workﬂows in the Cloud, pages 353–369. Springer International Publishing, Cham, 2018.",
            "reference_abstract": "Reinforcement Learning (RL) has demonstrated a great potential for\nautomatically solving decision-making problems in complex uncertain\nenvironments. RL proposes a computational approach that allows learning through\ninteraction in an environment with stochastic behavior, where agents take\nactions to maximize some cumulative short-term and long-term rewards. Some of\nthe most impressive results have been shown in Game Theory where agents\nexhibited superhuman performance in games like Go or Starcraft 2, which led to\nits gradual adoption in many other domains, including Cloud Computing.\nTherefore, RL appears as a promising approach for Autoscaling in Cloud since it\nis possible to learn transparent (with no human intervention), dynamic (no\nstatic plans), and adaptable (constantly updated) resource management policies\nto execute applications. These are three important distinctive aspects to\nconsider in comparison with other widely used autoscaling policies that are\ndefined in an ad-hoc way or statically computed as in solutions based on\nmeta-heuristics. Autoscaling exploits the Cloud elasticity to optimize the\nexecution of applications according to given optimization criteria, which\ndemands to decide when and how to scale-up/down computational resources, and\nhow to assign them to the upcoming processing workload. Such actions have to be\ntaken considering that the Cloud is a dynamic and uncertain environment.\nMotivated by this, many works apply RL to the autoscaling problem in the Cloud.\nIn this work, we survey exhaustively those proposals from major venues, and\nuniformly compare them based on a set of proposed taxonomies. We also discuss\nopen problems and prospective research in the area."
        },
        {
            "reference_num": "[45]",
            "reference_title": "Joonhyouk Jang, Jinman Jung, and Jiman Hong. k-LZF : An efﬁcient and fair scheduling for Edge Computing servers. Future Generation Computer Systems, 98:44–53, 2019.",
            "reference_abstract": "Scheduling is important in Edge computing. In contrast to the Cloud, Edge\nresources are hardware limited and cannot support workload-driven\ninfrastructure scaling. Hence, resource allocation and scheduling for the Edge\nrequires a fresh perspective. Existing Edge scheduling research assumes\navailability of all needed resources whenever a job request is made. This paper\nchallenges that assumption, since not all job requests from a Cloud server can\nbe scheduled on an Edge node. Thus, guaranteeing fairness among the clients\n(Cloud servers offloading jobs) while accounting for priorities of the jobs\nbecomes a critical task. This paper presents four scheduling techniques, the\nfirst is a naive first come first serve strategy and further proposes three\nstrategies, namely a client fair, priority fair, and hybrid that accounts for\nthe fairness of both clients and job priorities. An evaluation on a target\nplatform under three different scenarios, namely equal, random, and Gaussian\njob distributions is presented. The experimental studies highlight the low\noverheads and the distribution of scheduled jobs on the Edge node when compared\nto the naive strategy. The results confirm the superior performance of the\nhybrid strategy and showcase the feasibility of fair schedulers for Edge\ncomputing."
        },
        {
            "reference_num": "[46]",
            "reference_title": "Ziqian Dong, Ning Liu, and Roberto Rojas-Cessa. Greedy scheduling of tasks with time constraints for energy-efﬁcient cloud-computing data centers. Journal of Cloud Computing, 4, 2015.",
            "reference_abstract": "In this paper, we show a strong correlation between turnstile usage data of\nthe New York City subway provided by the Metropolitan Transport Authority of\nNew York City and COVID-19 deaths and cases reported by the New York City\nDepartment of Health. The turnstile usage data not only indicate the usage of\nthe city's subway but also people's activity that promoted the large prevalence\nof COVID-19 city dwellers experienced from March to May of 2020. While this\ncorrelation is apparent, no proof has been provided before. Here we demonstrate\nthis correlation through the application of a long short-term memory neural\nnetwork. We show that the correlation of COVID-19 prevalence and deaths\nconsiders the incubation and symptomatic phases on reported deaths. Having\nestablished this correlation, we estimate the dates when the number of COVID-19\ndeaths and cases would approach zero after the reported number of deaths were\ndecreasing by using the Auto-Regressive Integrated Moving Average model. We\nalso estimate the dates when the first cases and deaths occurred by\nback-tracing the data sets and compare them to the reported dates."
        },
        {
            "reference_num": "[47]",
            "reference_title": "Shaminder Kaur and Amandeep Verma. An Efﬁcient Approach to Genetic Algorithm for Task Scheduling in Cloud Computing Environment. International Journal of Information Technology and Computer Science, 4:74–79, 2012.",
            "reference_abstract": "With the raise in practice of Internet, in social, personal, commercial and\nother aspects of life, the cybercrime is as well escalating at an alarming\nrate. Such usage of Internet in diversified areas also augmented the illegal\nactivities, which in turn, bids many network attacks and threats. Network\nforensics is used to detect the network attacks. This can be viewed as the\nextension of network security. It is the technology, which detects and also\nsuggests prevention of the various network attacks. Botnet is one of the most\ncommon attacks and is regarded as a network of hacked computers. It captures\nthe network packet, store it and then analyze and correlate to find the source\nof attack. Various methods based on this approach for botnet detection are in\nliterature, but a generalized method is lacking. So, there is a requirement to\ndesign a generic framework that can be used by any botnet detection. This\nframework is of use for researchers, in the development of their own method of\nbotnet detection, by means of providing methodology and guidelines. In this\npaper, various prevalent methods of botnet detection are studied, commonalities\namong them are established and then a generalized model for the detection of\nbotnet is proposed. The proposed framework is described as UML diagrams."
        },
        {
            "reference_num": "[48]",
            "reference_title": "Fatos Xhafa and Ajith Abraham. A compendium of heuristic methods for scheduling in computational grids. In Emilio Corchado and Hujun Yin, editors, Intelligent Data Engineering and Automated Learning IDEAL 2009, pages 751–758, Berlin, Heidelberg, 2009. Springer Berlin Heidelberg.",
            "reference_abstract": "The role of operational quantum mechanics, quantum axiomatics and quantum\nstructures in general is presented as a contribution to a compendium on quantum\nphysics, its history and philosophy."
        },
        {
            "reference_num": "[49]",
            "reference_title": "Anton Beloglazov and Rajkumar Buyya. Optimal online deterministic algorithms and adaptive heuristics for energy and performance efﬁcient dynamic consolidation of virtual machines in Cloud data centers. Concurrency and Computation: Practice and Experience, 24:1397–1420, 2012.",
            "reference_abstract": "Underutilization of computing resources and high power consumption are two\nprimary challenges in the domain of Cloud resource management. This paper deals\nwith these challenges through offline, migration impact-aware, multi-objective\ndynamic Virtual Machine (VM) consolidation in the context of large-scale\nvirtualized data center environments. The problem is formulated as an NP-hard\ndiscrete combinatorial optimization problem with simultaneous objectives of\nminimizing resource wastage, power consumption, and the associated VM migration\noverhead. Since dynamic VM consolidation through VM live migrations have\nnegative impacts on hosted applications performance and data center components,\na VM live migration overhead estimation technique is proposed, which takes into\naccount pragmatic migration parameters and overhead factors. In order to tackle\nscalability issues, a hierarchical, decentralized dynamic VM consolidation\nframework is presented that helps to localize migration-related network traffic\nand reduce network cost. Moreover, a multi-objective, dynamic VM consolidation\nalgorithm is proposed by utilizing the Ant Colony Optimization (ACO)\nmetaheuristic, with integration of the proposed VM migration overhead\nestimation technique. Comprehensive performance evaluation makes it evident\nthat the proposed dynamic VM consolidation approach outpaces the\nstate-of-the-art offline, migration-aware dynamic VM consolidation algorithm\nacross all performance metrics by reducing the overall power consumption by up\nto 47%, resource wastage by up to 64%, and migration overhead by up to 83%."
        },
        {
            "reference_num": "[50]",
            "reference_title": "Kyong-Ha Lee, Yoon-Joon Lee, Hyunsik Choi, Yon Dohn Chung, and Bongki Moon. Parallel data processing with mapreduce: A survey. SIGMOD Rec., 40:11–20, January 2012.",
            "reference_abstract": "In recent times, the production of multidimensional data in various domains\nand their storage in array databases has witnessed a sharp increase; this rapid\ngrowth in data volumes necessitates compression in array databases. However,\nexisting compression schemes used in array databases are general-purpose and\nnot designed specifically for the databases. They could degrade query\nperformance with complex analytical tasks, which incur huge computing costs.\nThus, a compression scheme that considers the workflow of array databases is\nrequired. This study presents a compression scheme, SEACOW, for storing and\nquerying multidimensional array data. The scheme is specially designed to be\nefficient for both dimension-based and value-based exploration. It considers\ndata access patterns for exploration queries and embeds a synopsis, which can\nbe utilized as an index, in the compressed array. In addition, we implement an\narray storage system, namely MSDB, to perform experiments. We evaluate query\nperformance on real scientific datasets and compared it with those of existing\ncompression schemes. Finally, our experiments demonstrate that SEACOW provides\nhigh compression rates compared to existing compression schemes, and the\nsynopsis improves analytical query processing performance."
        },
        {
            "reference_num": "[51]",
            "reference_title": "Daria Glushkova, Petar Jovanovic, and Alberto Abelló. Mapreduce performance model for hadoop 2.x. Information Systems, 79:32–43, 2019. Special issue on DOLAP 2017: Design, Optimization, Languages and Analytical Processing of Big Data.",
            "reference_abstract": "Nowadays many companies have available large amounts of raw, unstructured\ndata. Among Big Data enabling technologies, a central place is held by the\nMapReduce framework and, in particular, by its open source implementation,\nApache Hadoop. For cost effectiveness considerations, a common approach entails\nsharing server clusters among multiple users. The underlying infrastructure\nshould provide every user with a fair share of computational resources,\nensuring that Service Level Agreements (SLAs) are met and avoiding wastes. In\nthis paper we consider two mathematical programming problems that model the\noptimal allocation of computational resources in a Hadoop 2.x cluster with the\naim to develop new capacity allocation techniques that guarantee better\nperformance in shared data centers. Our goal is to get a substantial reduction\nof power consumption while respecting the deadlines stated in the SLAs and\navoiding penalties associated with job rejections. The core of this approach is\na distributed algorithm for runtime capacity allocation, based on Game Theory\nmodels and techniques, that mimics the MapReduce dynamics by means of\ninteracting players, namely the central Resource Manager and Class Managers."
        },
        {
            "reference_num": "[52]",
            "reference_title": "Jingqi Yang, Chuanchang Liu, Yanlei Shang, Bo Cheng, Zexiang Mao, Chunhong Liu, Lisha Niu, and Junliang Chen. A cost-aware auto-scaling approach using the workload prediction in service clouds. Information Systems Frontiers, 16:7–18, 2014.",
            "reference_abstract": "Sensitive data protection is essential for mobile users. Plausibly Deniable\nEncryption (PDE) systems provide an effective manner to protect sensitive data\nby hiding them on the device. However, existing PDE systems can lose data due\nto overriding the hidden volume, waste physical storage because of the reserved\narea used for avoiding data loss, and require device reboot when using the\nhidden volume. This paper presents MobiGyges, a hidden volume-based mobile PDE\nsystem, to fill the gap. MobiGyges addresses the problem of data loss by\nrestricting each storage block used only by one volume, and it improves storage\nutilization by eliminating the reserved area. MobiGyges can also avoid device\nreboot by mounting the hidden volume dynamically on-demand with the Dynamic\nMounting service. Moreover, we identify two novel PDE oriented attacks, the\ncapacity comparison attack and the fill-to-full attack. MobiGyges can defend\nthem by jointly leveraging the Shrunk U-disk method and multi-level\ndeniability. We implement the MobiGyges proof-of-concept system on a real\nmobile phone Google Nexus 6P with LineageOS 13. Experimental results show that\nMobiGyges prevents data loss, avoids device reboot, improves storage\nutilization by over 30% with acceptable performance overhead compared with\ncurrent works."
        },
        {
            "reference_num": "[53]",
            "reference_title": "Nilabja Roy, Abhishek Dubey, and Aniruddha Gokhale. Efﬁcient autoscaling in the cloud using predictive models for workload forecasting. Proceedings 2011 IEEE 4th International Conference on Cloud Computing, CLOUD 2011, pages 500–507, 2011.",
            "reference_abstract": "Deep Learning (DL) model-based AI services are increasingly offered in a\nvariety of predictive analytics services such as computer vision, natural\nlanguage processing, speech recognition. However, the quality of the DL models\ncan degrade over time due to changes in the input data distribution, thereby\nrequiring periodic model updates. Although cloud data-centers can meet the\ncomputational requirements of the resource-intensive and time-consuming model\nupdate task, transferring data from the edge devices to the cloud incurs a\nsignificant cost in terms of network bandwidth and are prone to data privacy\nissues. With the advent of GPU-enabled edge devices, the DL model update can be\nperformed at the edge in a distributed manner using multiple connected edge\ndevices. However, efficiently utilizing the edge resources for the model update\nis a hard problem due to the heterogeneity among the edge devices and the\nresource interference caused by the co-location of the DL model update task\nwith latency-critical tasks running in the background. To overcome these\nchallenges, we present Deep-Edge, a load- and interference-aware,\nfault-tolerant resource management framework for performing model update at the\nedge that uses distributed training. This paper makes the following\ncontributions. First, it provides a unified framework for monitoring,\nprofiling, and deploying the DL model update tasks on heterogeneous edge\ndevices. Second, it presents a scheduler that reduces the total re-training\ntime by appropriately selecting the edge devices and distributing data among\nthem such that no latency-critical applications experience deadline violations.\nFinally, we present empirical results to validate the efficacy of the framework\nusing a real-world DL model update case-study based on the Caltech dataset and\nan edge AI cluster testbed."
        },
        {
            "reference_num": "[54]",
            "reference_title": "Mahmoud Al-Ayyoub, Yaser Jararweh, Mustafa Daraghmeh, and Qutaibah Althebyan. Multi-agent based dynamic resource provisioning and monitoring for cloud computing systems infrastructure. Cluster Computing, 18:919–932, 2015.  34",
            "reference_abstract": "A Hybrid cloud is an integration of resources between private and public\nclouds. It enables users to horizontally scale their on-premises infrastructure\nup to public clouds in order to improve performance and cut up-front investment\ncost. This model of applications deployment is called cloud bursting that\nallows data-intensive applications especially distributed database systems to\nhave the benefit of both private and public clouds. In this work, we present an\nautomated implementation of a hybrid cloud using (i) a robust and zero-cost\nLinux-based VPN to make a secure connection between private and public clouds,\nand (ii) Terraform as a software tool to deploy infrastructure resources based\non the requirements of hybrid cloud. We also explore performance evaluation of\ncloud bursting for six modern and distributed database systems on the hybrid\ncloud spanning over local OpenStack and Microsoft Azure. Our results reveal\nthat MongoDB and MySQL Cluster work efficient in terms of throughput and\noperations latency if they burst into a public cloud to supply their resources.\nIn contrast, the performance of Cassandra, Riak, Redis, and Couchdb reduces if\nthey significantly leverage their required resources via cloud bursting."
        },
        {
            "reference_num": "[55]",
            "reference_title": "Shair Horovitz and Yain Arian. Efﬁcient cloud auto-scaling with SLA Objective Using Q-Learning. In 6th International Conference on Future Internet of Things and Cloud (FiCloud), pages 85–92. IEEE, 2018.",
            "reference_abstract": "Internet of Things devices are envisioned to penetrate essentially all\naspects of life, including homes and urbanspaces, in use cases such as health\ncare, assisted living, and smart cities. One often proposed solution for\ndealing with the massive amount of data collected by these devices and offering\nservices on top of them is the federation of the Internet of Things and cloud\ncomputing. However, user acceptance of such systems is a critical factor that\nhinders the adoption of this promising approach due to severe privacy concerns.\nWe present UPECSI, an approach for user-driven privacy enforcement for\ncloud-based services in the Internet of Things to address this critical factor.\nUPECSI enables enforcement of all privacy requirements of the user once her\nsensitive data leaves the border of her network, provides a novel approach for\nthe integration of privacy functionality into the development process of\ncloud-based services, and offers the user an adaptable and transparent\nconfiguration of her privacy requirements. Hence, UPECSI demonstrates an\napproach for realizing user-accepted cloud services in the Internet of Things."
        },
        {
            "reference_num": "[56]",
            "reference_title": "Yi Wei, Daniel Kudenko, Shijun Liu, Li Pan, Lei Wu, and Xiangxiu Meng. A reinforcement learning based auto-scaling approach for saas providers in dynamic cloud environment. Mathematical Problems in Engineering, pages 1–11, 2019.",
            "reference_abstract": "In this paper, we study the market-oriented online bi-objective service\nscheduling problem for pleasingly parallel jobs with variable resources in\ncloud environments, from the perspective of SaaS (Software-as-as-Service)\nproviders who provide job-execution services. The main process of scheduling\nSaaS services in clouds is: a SaaS provider purchases cloud instances from IaaS\nproviders to schedule end users' jobs and charges users accordingly. This\nproblem has several particular features, such as the job-oriented end users,\nthe pleasingly parallel jobs with soft deadline constraints, the online\nsettings, and the variable numbers of resources. For maximizing both the\nrevenue and the user satisfaction rate, we design an online algorithm for SaaS\nproviders to optimally purchase IaaS instances and schedule pleasingly parallel\njobs. The proposed algorithm can achieve competitive objectives in polynomial\nrun-time. The theoretical analysis and simulations based on real-world Google\njob traces as well as synthetic datasets validate the effectiveness and\nefficiency of our algorithm."
        },
        {
            "reference_num": "[57]",
            "reference_title": "Seyed Mohammad Reza Nouri, Han Li, Srikumar Venugopal, Wenxia Guo, MingYun He, and Wenhong Tian. Autonomic decentralized elasticity based on a reinforcement learning controller for cloud applications. Future Generation Computer Systems, 94:765–780, 2019.",
            "reference_abstract": "With the advent of cloud computing, organizations are nowadays able to react\nrapidly to changing demands for computational resources. Not only individual\napplications can be hosted on virtual cloud infrastructures, but also complete\nbusiness processes. This allows the realization of so-called elastic processes,\ni.e., processes which are carried out using elastic cloud resources. Despite\nthe manifold benefits of elastic processes, there is still a lack of solutions\nsupporting them.\n  In this paper, we identify the state of the art of elastic Business Process\nManagement with a focus on infrastructural challenges. We conceptualize an\narchitecture for an elastic Business Process Management System and discuss\nexisting work on scheduling, resource allocation, monitoring, decentralized\ncoordination, and state management for elastic processes. Furthermore, we\npresent two representative elastic Business Process Management Systems which\nare intended to counter these challenges. Based on our findings, we identify\nopen issues and outline possible research directions for the realization of\nelastic processes and elastic Business Process Management."
        },
        {
            "reference_num": "[58]",
            "reference_title": "Gary Marcus. Deep Learning: A Critical Appraisal. arXiv e-prints, page arXiv:1801.00631, Jan 2018.",
            "reference_abstract": "Although deep learning has historical roots going back decades, neither the\nterm \"deep learning\" nor the approach was popular just over five years ago,\nwhen the field was reignited by papers such as Krizhevsky, Sutskever and\nHinton's now classic (2012) deep network model of Imagenet. What has the field\ndiscovered in the five subsequent years? Against a background of considerable\nprogress in areas such as speech recognition, image recognition, and game\nplaying, and considerable enthusiasm in the popular press, I present ten\nconcerns for deep learning, and suggest that deep learning must be supplemented\nby other techniques if we are to reach artificial general intelligence."
        },
        {
            "reference_num": "[59]",
            "reference_title": "Leslie N. Smith. A disciplined approach to neural network hyper-parameters: Part 1 – learning rate, batch size, momentum, and weight decay. arXiv e-prints, page arXiv:1803.09820, Mar 2018.",
            "reference_abstract": "Although deep learning has produced dazzling successes for applications of\nimage, speech, and video processing in the past few years, most trainings are\nwith suboptimal hyper-parameters, requiring unnecessarily long training times.\nSetting the hyper-parameters remains a black art that requires years of\nexperience to acquire. This report proposes several efficient ways to set the\nhyper-parameters that significantly reduce training time and improves\nperformance. Specifically, this report shows how to examine the training\nvalidation/test loss function for subtle clues of underfitting and overfitting\nand suggests guidelines for moving toward the optimal balance point. Then it\ndiscusses how to increase/decrease the learning rate/momentum to speed up\ntraining. Our experiments show that it is crucial to balance every manner of\nregularization for each dataset and architecture. Weight decay is used as a\nsample regularizer to show how its optimal value is tightly coupled with the\nlearning rates and momentums. Files to help replicate the results reported here\nare available."
        },
        {
            "reference_num": "[60]",
            "reference_title": "Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural Computation, 9:1735–1780, 1997.",
            "reference_abstract": "Recurrent neural network is a powerful model that learns temporal patterns in\nsequential data. For a long time, it was believed that recurrent networks are\ndifficult to train using simple optimizers, such as stochastic gradient\ndescent, due to the so-called vanishing gradient problem. In this paper, we\nshow that learning longer term patterns in real data, such as in natural\nlanguage, is perfectly possible using gradient descent. This is achieved by\nusing a slight structural modification of the simple recurrent neural network\narchitecture. We encourage some of the hidden units to change their state\nslowly by making part of the recurrent weight matrix close to identity, thus\nforming kind of a longer term memory. We evaluate our model in language\nmodeling experiments, where we obtain similar performance to the much more\ncomplex Long Short Term Memory (LSTM) networks (Hochreiter &amp; Schmidhuber,\n1997)."
        },
        {
            "reference_num": "[61]",
            "reference_title": "Hongjia Li, J. Li, Wang Yao, S. Nazarian, X. Lin, and Y. Wang. Fast and energy-aware resource provisioning and task scheduling for cloud systems. In 2017 18th International Symposium on Quality Electronic Design (ISQED), pages 174–179, March 2017.",
            "reference_abstract": "The recent breakthroughs of deep reinforcement learning (DRL) technique in\nAlpha Go and playing Atari have set a good example in handling large state and\nactions spaces of complicated control problems. The DRL technique is comprised\nof (i) an offline deep neural network (DNN) construction phase, which derives\nthe correlation between each state-action pair of the system and its value\nfunction, and (ii) an online deep Q-learning phase, which adaptively derives\nthe optimal action and updates value estimates. In this paper, we first present\nthe general DRL framework, which can be widely utilized in many applications\nwith different optimization objectives. This is followed by the introduction of\nthree specific applications: the cloud computing resource allocation problem,\nthe residential smart grid task scheduling problem, and building HVAC system\noptimal control problem. The effectiveness of the DRL technique in these three\ncyber-physical applications have been validated. Finally, this paper\ninvestigates the stochastic computing-based hardware implementations of the DRL\nframework, which consumes a significant improvement in area efficiency and\npower consumption compared with binary-based implementation counterparts."
        },
        {
            "reference_num": "[62]",
            "reference_title": "Zhiguang Wang, Chul Gwon, Tim Oates, and Adam Iezzi. Automated Cloud Provisioning on AWS using Deep Reinforcement Learning. arXiv e-prints, page arXiv:1709.04305, Sep 2017.",
            "reference_abstract": "As the use of cloud computing continues to rise, controlling cost becomes\nincreasingly important. Yet there is evidence that 30\\% - 45\\% of cloud spend\nis wasted. Existing tools for cloud provisioning typically rely on highly\ntrained human experts to specify what to monitor, thresholds for triggering\naction, and actions. In this paper we explore the use of reinforcement learning\n(RL) to acquire policies to balance performance and spend, allowing humans to\nspecify what they want as opposed to how to do it, minimizing the need for\ncloud expertise. Empirical results with tabular, deep, and dueling double deep\nQ-learning with the CloudSim simulator show the utility of RL and the relative\nmerits of the approaches. We also demonstrate effective policy transfer\nlearning from an extremely simple simulator to CloudSim, with the next step\nbeing transfer from CloudSim to an Amazon Web Services physical environment."
        },
        {
            "reference_num": "[63]",
            "reference_title": "Pravesh Humane and JN Varshapriya. Simulation of cloud infrastructure using cloudsim simulator: A practical approach for researchers. In 2015 International Conference on Smart Technologies and Management for Computing, Communication, Controls, Energy and Materials (ICSTM), pages 207– 211. IEEE, 2015.",
            "reference_abstract": "Cloud computing environment simulators enable cost-effective experimentation\nof novel infrastructure designs and management approaches by avoiding\nsignificant costs incurred from repetitive deployments in real Cloud platforms.\nHowever, widely used Cloud environment simulators compromise on usability due\nto complexities in design and configuration, along with the added overhead of\nprogramming language expertise. Existing approaches attempting to reduce this\noverhead, such as script-based simulators and Graphical User Interface (GUI)\nbased simulators, often compromise on the extensibility of the simulator.\nSimulator extensibility allows for customization at a fine-grained level, thus\nreducing it significantly affects flexibility in creating simulations. To\naddress these challenges, we propose an architectural framework to enable\nhuman-readable script-based simulations in existing Cloud environment\nsimulators while minimizing the impact on simulator extensibility. We implement\nthe proposed framework for the widely used Cloud environment simulator, the\nCloudSim toolkit, and compare it against state-of-the-art baselines using a\npractical use case. The resulting framework, called CloudSim Express, achieves\nextensible simulations while surpassing baselines with over a 71.43% reduction\nin code complexity and an 89.42% reduction in lines of code."
        },
        {
            "reference_num": "[64]",
            "reference_title": "Ziyu Wang, Tom Schaul, Matteo Hessel, Hado van Hasselt, Marc Lanctot, and Nando de Freitas. Dueling Network Architectures for Deep Reinforcement Learning. arXiv e-prints, page arXiv:1511.06581, Nov 2015.",
            "reference_abstract": "In recent years there have been many successes of using deep representations\nin reinforcement learning. Still, many of these applications use conventional\narchitectures, such as convolutional networks, LSTMs, or auto-encoders. In this\npaper, we present a new neural network architecture for model-free\nreinforcement learning. Our dueling network represents two separate estimators:\none for the state value function and one for the state-dependent action\nadvantage function. The main benefit of this factoring is to generalize\nlearning across actions without imposing any change to the underlying\nreinforcement learning algorithm. Our results show that this architecture leads\nto better policy evaluation in the presence of many similar-valued actions.\nMoreover, the dueling architecture enables our RL agent to outperform the\nstate-of-the-art on the Atari 2600 domain."
        },
        {
            "reference_num": "[65]",
            "reference_title": "Zhao Tong, Hongjian Chen, Xiaomei Deng, Kenli Li, and Keqin Li. A scheduling scheme in the cloud computing environment using deep Q-learning. Information Sciences, 512:1170 – 1191, 2020.",
            "reference_abstract": "In this paper, we propose a Distributed Intelligent Video Surveillance (DIVS)\nsystem using Deep Learning (DL) algorithms and deploy it in an edge computing\nenvironment. We establish a multi-layer edge computing architecture and a\ndistributed DL training model for the DIVS system. The DIVS system can migrate\ncomputing workloads from the network center to network edges to reduce huge\nnetwork communication overhead and provide low-latency and accurate video\nanalysis solutions. We implement the proposed DIVS system and address the\nproblems of parallel training, model synchronization, and workload balancing.\nTask-level parallel and model-level parallel training methods are proposed to\nfurther accelerate the video analysis process. In addition, we propose a model\nparameter updating method to achieve model synchronization of the global DL\nmodel in a distributed EC environment. Moreover, a dynamic data migration\napproach is proposed to address the imbalance of workload and computational\npower of edge nodes. Experimental results showed that the EC architecture can\nprovide elastic and scalable computing power, and the proposed DIVS system can\nefficiently handle video surveillance and analysis tasks."
        },
        {
            "reference_num": "[66]",
            "reference_title": "Md Golam Rabiul Alam, Mohammad Hassan, Zla Uddin, Ahmad Almogren, and Giancarlo Fortino. Autonomic computation ofﬂoading in mobile edge for IoT applications. Future Generation Computer Systems, 90:149–157, 2019.",
            "reference_abstract": "For a full-stack web or app development, it requires a software firm or more\nspecifically a team of experienced developers to contribute a large portion of\ntheir time and resources to design the website and then convert it to code. As\na result, the efficiency of the development team is significantly reduced when\nit comes to converting UI wireframes and database schemas into an actual\nworking system. It would save valuable resources and fasten the overall\nworkflow if the clients or developers can automate this process of converting\nthe pre-made full-stack website design to get a partially working if not fully\nworking code. In this paper, we present a novel approach of generating the\nskeleton code from sketched images using Deep Learning and Computer Vision\napproaches. The dataset for training are first-hand sketched images of low\nfidelity wireframes, database schemas and class diagrams. The approach consists\nof three parts. First, the front-end or UI elements detection and extraction\nfrom custom-made UI wireframes. Second, individual database table creation from\nschema designs and lastly, creating a class file from class diagrams."
        },
        {
            "reference_num": "[67]",
            "reference_title": "Bingqian Du, Chuan Wu, and Zhiyi Huang. Learning resource allocation and pricing for cloud proﬁt maximization. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 33, pages 7570–7577, Honolulu,Hawaii, USA, 2019. AAAI Press.",
            "reference_abstract": "This volume represents the accepted submissions from the AAAI-2019 Workshop\non Games and Simulations for Artificial Intelligence held on January 29, 2019\nin Honolulu, Hawaii, USA. https://www.gamesim.ai"
        },
        {
            "reference_num": "[68]",
            "reference_title": "Jia Rao, Xiangping Bu, Cheng-Zhong Xu, Leyi Wang, and George Yin. Vconf: A reinforcement learning approach to virtual machines auto-conﬁguration. In Proceedings of the 6th International Conference on Autonomic Computing, ICAC ’09, pages 137–146, New York, NY, USA, 2009. ACM.",
            "reference_abstract": "It is common in the internet industry to use offline-developed algorithms to\npower online products that contribute to the success of a business.\nOffline-developed algorithms are guided by offline evaluation metrics, which\nare often different from online business key performance indicators (KPIs). To\nmaximize business KPIs, it is important to pick a north star among all\navailable offline evaluation metrics. By noting that online products can be\nmeasured by online evaluation metrics, the online counterparts of offline\nevaluation metrics, we decompose the problem into two parts. As the offline A/B\ntest literature works out the first part: counterfactual estimators of offline\nevaluation metrics that move the same way as their online counterparts, we\nfocus on the second part: causal effects of online evaluation metrics on\nbusiness KPIs. The north star of offline evaluation metrics should be the one\nwhose online counterpart causes the most significant lift in the business KPI.\nWe model the online evaluation metric as a mediator and formalize its causality\nwith the business KPI as dose-response function (DRF). Our novel approach,\ncausal meta-mediation analysis, leverages summary statistics of many existing\nrandomized experiments to identify, estimate, and test the mediator DRF. It is\neasy to implement and to scale up, and has many advantages over the literature\nof mediation analysis and meta-analysis. We demonstrate its effectiveness by\nsimulation and implementation on real data."
        },
        {
            "reference_num": "[70]",
            "reference_title": "Nicos Makris. Plastic torsional buckling of cruciform compression members. Journal of Engineering Mechanics, 129:689–696, 2003.  35",
            "reference_abstract": "Inverted repeat (IR) sequences in DNA can form non-canonical cruciform\nstructures to relieve torsional stress. We use Monte Carlo simulations of a\nrecently developed coarse-grained model of DNA to demonstrate that the\nnucleation of a cruciform can proceed through a cooperative mechanism. Firstly,\na twist-induced denaturation bubble must diffuse so that its midpoint is near\nthe centre of symmetry of the IR sequence. Secondly, bubble fluctuations must\nbe large enough to allow one of the arms to form a small number of hairpin\nbonds. Once the first arm is partially formed, the second arm can rapidly grow\nto a similar size. Because bubbles can twist back on themselves, they need\nconsiderably fewer bases to resolve torsional stress than the final cruciform\nstate does. The initially stabilised cruciform therefore continues to grow,\nwhich typically proceeds synchronously, reminiscent of the S-type mechanism of\ncruciform formation. By using umbrella sampling techniques we calculate, for\ndifferent temperatures and superhelical densities, the free energy as a\nfunction of the number of bonds in each cruciform along the correlated but\nnon-synchronous nucleation pathways we observed in direct simulations."
        },
        {
            "reference_num": "[71]",
            "reference_title": "Carlos García Garino, Melisa Ribero Vairo, Susana Andía Fagés, Anibal Mirasso, and Jean-Phillip Ponthot. Numerical simulation of ﬁnite strain viscoplastic problems. Journal of Computational and Applied Mathematics, 246:174–184, July 2013.",
            "reference_abstract": "Numerical simulations of the flow in an extrusion damper are performed using\na finite volume method. The damper is assumed to consist of a shaft, with or\nwithout a spherical bulge, oscillating axially in a containing cylinder filled\nwith a viscoplastic material of Bingham type. The response of the damper to a\nforced sinusoidal displacement is studied. In the bulgeless case the\nconfiguration is the annular analogue of the well-known lid-driven cavity\nproblem, but with a sinusoidal rather than constant lid velocity. Navier slip\nis applied to the shaft surface in order to bound the reaction force to finite\nvalues. Starting from a base case, several problem parameters are varied in\nturn in order to study the effects of viscoplasticity, slip, damper geometry\nand oscillation frequency to the damper response. The results show that,\ncompared to Newtonian flow, viscoplasticity causes the damper force to be less\nsensitive to the shaft velocity; this is often a desirable damper property. The\nbulge increases the required force on the damper mainly by generating a\npressure difference across itself; the latter is larger the smaller the gap\nbetween the bulge and the casing is. At high yield stresses or slip\ncoefficients the amount of energy dissipation that occurs due to sliding\nfriction at the shaft-fluid interface is seen to increase significantly. At low\nfrequencies the flow is in quasi steady state, dominated by viscoplastic\nforces, while at higher frequencies the fluid kinetic energy storage and\nrelease also come into the energy balance, introducing hysteresis effects."
        },
        {
            "reference_num": "[72]",
            "reference_title": "Saurabh Kumar Garg, Chee Shin Yeo, and Rajkumar Buyya. Green cloud framework for improving carbon efﬁciency of clouds. In Emmanuel Jeannot, Raymond Namyst, and Jean Roman, editors, EuroPar 2011 Parallel Processing, pages 491–502, Berlin, Heidelberg, 2011. Springer Berlin Heidelberg.",
            "reference_abstract": "The use of High Performance Computing (HPC) in commercial and consumer IT\napplications is becoming popular. They need the ability to gain rapid and\nscalable access to high-end computing capabilities. Cloud computing promises to\ndeliver such a computing infrastructure using data centers so that HPC users\ncan access applications and data from a Cloud anywhere in the world on demand\nand pay based on what they use. However, the growing demand drastically\nincreases the energy consumption of data centers, which has become a critical\nissue. High energy consumption not only translates to high energy cost, which\nwill reduce the profit margin of Cloud providers, but also high carbon\nemissions which is not environmentally sustainable. Hence, energy-efficient\nsolutions are required that can address the high increase in the energy\nconsumption from the perspective of not only Cloud provider but also from the\nenvironment. To address this issue we propose near-optimal scheduling policies\nthat exploits heterogeneity across multiple data centers for a Cloud provider.\nWe consider a number of energy efficiency factors such as energy cost, carbon\nemission rate, workload, and CPU power efficiency which changes across\ndifferent data center depending on their location, architectural design, and\nmanagement system. Our carbon/energy based scheduling policies are able to\nachieve on average up to 30% of energy savings in comparison to profit based\nscheduling policies leading to higher profit and less carbon emissions."
        },
        {
            "reference_num": "[73]",
            "reference_title": "Anton Beloglazov, Rajkumar Buyya, Young Choon Lee, and Albert Zomaya. A Taxonomy and Survey of Energy-Efﬁcient Data Centers and Cloud Computing Systems, volume 82. Elsevier Inc., 1 edition, 2011.",
            "reference_abstract": "Traditionally, the development of computing systems has been focused on\nperformance improvements driven by the demand of applications from consumer,\nscientific and business domains. However, the ever increasing energy\nconsumption of computing systems has started to limit further performance\ngrowth due to overwhelming electricity bills and carbon dioxide footprints.\nTherefore, the goal of the computer system design has been shifted to power and\nenergy efficiency. To identify open challenges in the area and facilitate\nfuture advancements it is essential to synthesize and classify the research on\npower and energy-efficient design conducted to date. In this work we discuss\ncauses and problems of high power / energy consumption, and present a taxonomy\nof energy-efficient design of computing systems covering the hardware,\noperating system, virtualization and data center levels. We survey various key\nworks in the area and map them to our taxonomy to guide future design and\ndevelopment efforts. This chapter is concluded with a discussion of\nadvancements identified in energy-efficient computing and our vision on future\nresearch directions."
        },
        {
            "reference_num": "[74]",
            "reference_title": "Chunming Liu, Xin Xu, and Dewen Hu. Multiobjective reinforcement learning: A comprehensive overview. IEEE Transactions on Systems, Man, and Cybernetics: Systems, 45:385–398, March 2015.",
            "reference_abstract": "Reinforcement learning algorithms have performed well in playing challenging\nboard and video games. More and more studies focus on improving the\ngeneralisation ability of reinforcement learning algorithms. The General Video\nGame AI Learning Competition aims to develop agents capable of learning to play\ndifferent game levels that were unseen during training. This paper summarises\nthe five years' General Video Game AI Learning Competition editions. At each\nedition, three new games were designed. The training and test levels were\ndesigned separately in the first three editions. Since 2020, three test levels\nof each game were generated by perturbing or combining two training levels.\nThen, we present a novel reinforcement learning technique with dual-observation\nfor general video game playing, assuming that it is more likely to observe\nsimilar local information in different levels rather than global information.\nInstead of directly inputting a single, raw pixel-based screenshot of the\ncurrent game screen, our proposed general technique takes the encoded,\ntransformed global and local observations of the game screen as two\nsimultaneous inputs, aiming at learning local information for playing new\nlevels. Our proposed technique is implemented with three state-of-the-art\nreinforcement learning algorithms and tested on the game set of the 2020\nGeneral Video Game AI Learning Competition. Ablation studies show the\noutstanding performance of using encoded, transformed global and local\nobservations as input."
        },
        {
            "reference_num": "[76]",
            "reference_title": "and State-Action-Reward-State-Action (SARSA). It is important to highlight that one of the main limitations in RL is that the convergence time of these algorithms depends directly on the dimension of the state space and actions. Moreover, since these algorithms do not have an adequate initial policy they have a poor initial performance that will have  37  a greater or lesser impact depending on the addressed problem and the time taken for training. Making inappropriate decisions at the beginning of the autoscaling of workﬂows in Cloud, which is necessary for the exploration process, can have a direct impact on the makespan and the economic cost, so it is convenient to have an acceptable initial policy. This could also reduce the time required to learn the right policy. In any case, it is necessary to consider that obtaining an acceptable initial policy is not always trivial",
            "reference_abstract": "Infinite-state Markov Decision Processes (MDPs) are essential in modeling and\noptimizing a wide variety of engineering problems. In the reinforcement\nlearning (RL) context, a variety of algorithms have been developed to learn and\noptimize these MDPs. At the heart of many popular policy-gradient based\nlearning algorithms, such as natural actor-critic, TRPO, and PPO, lies the\nNatural Policy Gradient (NPG) algorithm. Convergence results for these RL\nalgorithms rest on convergence results for the NPG algorithm. However, all\nexisting results on the convergence of the NPG algorithm are limited to\nfinite-state settings.\n  We prove the first convergence rate bound for the NPG algorithm for\ninfinite-state average-reward MDPs, proving a $O(1/\\sqrt{T})$ convergence rate,\nif the NPG algorithm is initialized with a good initial policy. Moreover, we\nshow that in the context of a large class of queueing MDPs, the MaxWeight\npolicy suffices to satisfy our initial-policy requirement and achieve a\n$O(1/\\sqrt{T})$ convergence rate. Key to our result are state-dependent bounds\non the relative value function achieved by the iterate policies of the NPG\nalgorithm."
        },
        {
            "reference_num": "[77]",
            "reference_title": "Christoph Dann, Gerhard Neumann, and Jan Peters. Policy evaluation with temporal differences: A survey and comparison. Journal of Machine Learning Research, 15:809–883, 2014.",
            "reference_abstract": "The performance of a reinforcement learning algorithm can vary drastically\nduring learning because of exploration. Existing algorithms provide little\ninformation about the quality of their current policy before executing it, and\nthus have limited use in high-stakes applications like healthcare. We address\nthis lack of accountability by proposing that algorithms output policy\ncertificates. These certificates bound the sub-optimality and return of the\npolicy in the next episode, allowing humans to intervene when the certified\nquality is not satisfactory. We further introduce two new algorithms with\ncertificates and present a new framework for theoretical analysis that\nguarantees the quality of their policies and certificates. For tabular MDPs, we\nshow that computing certificates can even improve the sample-efficiency of\noptimism-based exploration. As a result, one of our algorithms is the first to\nachieve minimax-optimal PAC bounds up to lower-order terms, and this algorithm\nalso matches (and in some settings slightly improves upon) existing minimax\nregret bounds."
        },
        {
            "reference_num": "[78]",
            "reference_title": "Pooyan Jamshidi, Amir Shariﬂoo, Claus Pahl, Hamid Arabnejad, Andreas Metzger, and Giovani Estrada. Fuzzy self-learning controllers for elasticity management in dynamic cloud architectures. Proceedings 2016 12th International ACM SIGSOFT Conference on Quality of Software Architectures, QoSA 2016, pages 70–79, 2016.  A.1 MDP Resolution via Dynamic Programming  Dynamic programming (DP) in this context refers to a collection of algorithms that can be used to compute optimal policies given a perfect model of the environment as an MDP. Methods based on DP compute the policy based on a complete model of the environment (Model-based). In the process of estimating the state values V (s), the probability distribution of the transitions between the states Pa(s, s′) is used. This often becomes a limitation, since it is not always possible to derive such a model. In some cases, the probability distribution of the transitions is estimated from data obtained in previous experiences. The DP methods offer an ofﬂine learning variant, where the policy is obtained by iterating over the model and not based on the dynamics of current experiences. It is important to note that prior estimates of other states are used in the process of estimating the values of the states (a technique known as bootstrap). Two widely used DP algorithms are policyIteration and valueIteration. Both algorithms have polynomial complexity in the number of states and actions, so it is important to consider the dimensions of these spaces when using DP. However, the search performed with DP is much more efﬁcient than an exhaustive exploration in the space of all possible policies. The policyIteration algorithm (see Algorithm 1) is deﬁned based on the iterative repetition of the evaluation and the improvement of the policy until convergence is achieved. In this way, the algorithm generates the following sequence of value functions and policies v0 → π0 → v1 → π1... → π∗ until to reach an appropriate policy. On the other hand, the valueIteration algorithm (see Algorithm 2), ﬁrst includes the search for the appropriate value function and then, the computation of the associated policy. These steps are not repeated because once the value function is adequate, so is the associated policy. The search for the appropriate value function can be understood as a combination of the policy improvement process and a truncated evaluation of the policy (the values are reassigned after a single sweep of the states) without losing convergence",
            "reference_abstract": "Direct policy search (DPS) and look-ahead tree (LT) policies are two widely\nused classes of techniques to produce high performance policies for sequential\ndecision-making problems. To make DPS approaches work well, one crucial issue\nis to select an appropriate space of parameterized policies with respect to the\ntargeted problem. A fundamental issue in LT approaches is that, to take good\ndecisions, such policies must develop very large look-ahead trees which may\nrequire excessive online computational resources. In this paper, we propose a\nnew hybrid policy learning scheme that lies at the intersection of DPS and LT,\nin which the policy is an algorithm that develops a small look-ahead tree in a\ndirected way, guided by a node scoring function that is learned through DPS.\nThe LT-based representation is shown to be a versatile way of representing\npolicies in a DPS scheme, while at the same time, DPS enables to significantly\nreduce the size of the look-ahead trees that are required to take high-quality\ndecisions.\n  We experimentally compare our method with two other state-of-the-art DPS\ntechniques and four common LT policies on four benchmark domains and show that\nit combines the advantages of the two techniques from which it originates. In\nparticular, we show that our method: (1) produces overall better performing\npolicies than both pure DPS and pure LT policies, (2) requires a substantially\nsmaller number of policy evaluations than other DPS techniques, (3) is easy to\ntune and (4) results in policies that are quite robust with respect to\nperturbations of the initial conditions."
        },
        {
            "reference_num": "[78]",
            "reference_title": ".  Motivated by this beneﬁt, some authors [8, 7] have proposed approaches based on FRL for autoscaling in Cloud. Figure 9 shows the interaction between the components involved in these approaches. First, the Cloud platform and the running application that composes the environment, which is continuously observed by a monitoring process. The monitoring process retrieves data of interest in the state of the environment and reports it to the Automatic Controller (AC). One of the main components of the AC is the FL-based control process called Fuzzy Controller (FC). The FC is composed of the Knowledge Base (or rules), the Fuzziﬁer, the Inference Engine and the Defuzziﬁer. In this way, the FC receives the signal of the environment state, transforms it to its diffuse representation, reasons based on the rules, and obtains a diffuse output that is ﬁnally returned to its clear representation. This output or action is used by the actuator process to modify the environment. The second component of the FC is precisely the RL process, which also receives the signal of the environment state and, guided by the optimization objectives, is responsible for learning the most appropriate set of rules to update the knowledge base of FC. Each member of the table of values Q is assigned to a speciﬁc rule (which describes some action-state pairs). Then, these values are updated during the learning process. In this way, it is possible to take advantage of the strengths of RL and FL strategies to design an automatic controller capable of evolving fuzzy rules that allow making approximate reasoning.  40",
            "reference_abstract": "In response to the increasing volume and sensitivity of data, traditional\ncentralized computing models face challenges, such as data security breaches\nand regulatory hurdles. Federated Computing (FC) addresses these concerns by\nenabling collaborative processing without compromising individual data privacy.\nThis is achieved through a decentralized network of devices, each retaining\ncontrol over its data, while participating in collective computations. The\nmotivation behind FC extends beyond technical considerations to encompass\nsocietal implications. As the need for responsible AI and ethical data\npractices intensifies, FC aligns with the principles of user empowerment and\ndata sovereignty. FC comprises of Federated Learning (FL) and Federated\nAnalytics (FA). FC systems became more complex over time and they currently\nlack a clear definition and taxonomy describing its moving pieces. Current\nsurveys capture domain-specific FL use cases, describe individual components in\nan FC pipeline individually or decoupled from each other, or provide a\nquantitative overview of the number of published papers. This work surveys more\nthan 150 papers to distill the underlying structure of FC systems with their\nbasic building blocks, extensions, architecture, environment, and motivation.\nWe capture FL and FA systems individually and point out unique difference\nbetween those two."
        }
    ]
}